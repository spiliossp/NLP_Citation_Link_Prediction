{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac82c33-4cfe-4464-979a-5ba51da3d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated FeatureExtractor with SciBERT and LDA topic paths\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import time\n",
    "from itertools import combinations, product\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class to organize and compute link prediction features for citation graphs.\n",
    "    Organized feature extraction with incremental flush to disk and INFO logging.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_path, output_dir, chunked=True, its=100000):\n",
    "        # Paths for resources\n",
    "        self.base_path = base_path\n",
    "        self.split_path = f\"{base_path}/split_train_val/citation_pairs_split_train_val.csv\"\n",
    "        self.authors_path = f\"{base_path}/paper_to_authors.pkl\"\n",
    "        \n",
    "        self.abstracts_emb_path = r\"D:/NLP/citation_link_prediction/abstracts_embeds.npy\"\n",
    "        self.tfidf_idx_path = f\"{base_path}/tfidf_pid_to_idx.pkl\"\n",
    "        \n",
    "        # αντί για pickle.load, θα φορτώσουμε .npy\n",
    "        self.specter_path = r'D:\\NLP\\citation_link_prediction\\specter_pretrained.npy'\n",
    "      \n",
    "        self.scibert_path = \"D:/NLP/data/paper_scibert_embeddings.pkl\"\n",
    "        self.bertopic_path = f\"{base_path}/bertopic_features.parquet\"\n",
    "        self.lda_topics_path = f\"{base_path}/paper_topics.parquet\"\n",
    "        \n",
    "\n",
    "        \n",
    "        # Output config\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.chunked = chunked\n",
    "        self.its = its  # estimated iterations per second for timing\n",
    "        \n",
    "        # Data holders\n",
    "        self.df_pairs = None\n",
    "        self.paper_to_authors = None\n",
    "        \n",
    "        self.specter_emb  = None\n",
    "        \n",
    "        self.pid_to_idx = None\n",
    "        self.specter_dict = None\n",
    "        self.scibert_dict = None\n",
    "        self.lda_topics = None\n",
    "        self.G = None\n",
    "        self.G_auth = None\n",
    "        \n",
    "        # will hold in-memory features if not chunked\n",
    "        self.feat_dict = {}  \n",
    "    \n",
    "    def load_data(self):\n",
    "        # 1. Load citation pairs\n",
    "        \"\"\"\n",
    "        Load citation pairs (train/val or test), paper→authors, TF–IDF matrix & index,\n",
    "        SPECTER & SciBERT embeddings, BERTopic & LDA topics, and build graphs.\n",
    "        \"\"\"\n",
    "        print(\"INFO: Loading data...\")\n",
    "        t0 = time.time()\n",
    "        # a) Citation pairs\n",
    "        path = self.split_path\n",
    "        self.df_pairs = pd.read_csv(\n",
    "            path, usecols=[\"citing\",\"cited\",\"split\",\"label\"],\n",
    "            dtype={\"citing\":int,\"cited\":int,\"split\":str,\"label\":int}\n",
    "        )\n",
    "        if 'split' in self.df_pairs.columns:\n",
    "            self.df_pairs = (\n",
    "                self.df_pairs[self.df_pairs[\"split\"].isin([\"train\",\"val\"])]\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "        print(f\"INFO: Loaded {len(self.df_pairs)} pairs in {time.time()-t0:.1f}s\")\n",
    "        \n",
    "        # b) Paper→authors mapping\n",
    "        with open(self.authors_path, \"rb\") as f:\n",
    "            self.paper_to_authors = pickle.load(f)\n",
    "        print(f\"INFO: Loaded authors mapping ({len(self.paper_to_authors)} papers)\")\n",
    "        \n",
    "        # c) TF–IDF index & matrix\n",
    "        # ► i) Load precomputed SVD embeddings for abstracts & authors\n",
    "        #(My tfidf instead of Kozel)\n",
    "        # load abstracts SVD embeddings (LSA 32d)\n",
    "        self.emb_abs = np.load(self.abstracts_emb_path)\n",
    "        # emb_abs.shape == (n_rows_in_tfidf_matrix, 32)\n",
    "        print(f\"INFO: Loaded abstracts_embeds {self.emb_abs.shape}\")\n",
    "\n",
    "        # μέσα στο load_data(), μετά το \"LOAD TF-IDF index\"\n",
    "        with open(self.tfidf_idx_path, \"rb\") as f:\n",
    "            self.pid_to_idx = pickle.load(f)\n",
    "        print(f\"INFO: Loaded pid_to_idx ({len(self.pid_to_idx)} entries)\")\n",
    "      \n",
    "        \n",
    "        # d) SPECTER embeddings\n",
    "        # Load Specter embeddings from .npy (shape = [n_papers_used, D_s])\n",
    "        self.specter_emb = np.load(self.specter_path)\n",
    "        print(f\"INFO: Loaded Specter embeddings array {self.specter_emb.shape}\")\n",
    "                \n",
    "        # e) SciBERT embeddings\n",
    "        with open(self.scibert_path, \"rb\") as f:\n",
    "            self.scibert_dict = pickle.load(f)\n",
    "        print(f\"INFO: Loaded SciBERT embeddings ({len(self.scibert_dict)})\")\n",
    "        \n",
    "        # f) BERTopic & LDA topics\n",
    "        self.lda_topics = pd.read_parquet(self.lda_topics_path)\n",
    "        print(f\"INFO: Loaded LDA topics ({len(self.lda_topics)}) in {time.time()-t0:.1f}s total\")\n",
    "\n",
    "        ## EMBEDS\n",
    "        with open(os.path.join(self.base_path,\n",
    "                               \"split_train_val\",\n",
    "                               \"citation_node2vec_tuned.pkl\"), \"rb\") as f:\n",
    "            self.citation_node2vec = pickle.load(f)\n",
    "        with open(os.path.join(self.base_path,\n",
    "                               \"split_train_val\",\n",
    "                               \"citation_walklets_weighted_undirected.pkl\"), \"rb\") as f:\n",
    "            self.citation_walklets = pickle.load(f)\n",
    "        with open(os.path.join(self.base_path, \"author_node2vec.pkl\"), \"rb\") as f:\n",
    "            self.author_node2vec = pickle.load(f)\n",
    "        with open(os.path.join(self.base_path, \"author_walklets.pkl\"), \"rb\") as f:\n",
    "            self.author_walklets = pickle.load(f)\n",
    "        print(f\"INFO: Loaded graph embeddings: \"\n",
    "              f\"citation_node2vec={len(self.citation_node2vec)}, \"\n",
    "              f\"citation_walklets={len(self.citation_walklets)}, \"\n",
    "              f\"author_node2vec={len(self.author_node2vec)}, \"\n",
    "              f\"author_walklets={len(self.author_walklets)}\")\n",
    "\n",
    "        # ——————————————————————————————————————————————————————————————\n",
    "        # ► h) Load BERTopic features and build topic‐dicts once for all functions\n",
    "        df_bt = pd.read_parquet(self.bertopic_path)\n",
    "        # dominant topic & entropy\n",
    "        self.topic_dict   = dict(zip(df_bt.paper_id, df_bt.bertopic_dominant_topic))\n",
    "        self.entropy_dict = dict(zip(df_bt.paper_id, df_bt.bertopic_topic_entropy))\n",
    "        # full distribution vectors\n",
    "        topic_cols = [c for c in df_bt.columns if c.startswith(\"topic_dist_\")]\n",
    "        td_df = df_bt.set_index(\"paper_id\")[topic_cols]\n",
    "        self.topic_dist_arr = {\n",
    "            pid: td_df.loc[pid].to_numpy()\n",
    "            for pid in td_df.index\n",
    "        }\n",
    "        # per‐author average topic vector\n",
    "        from collections import defaultdict\n",
    "        author_topic_acc = defaultdict(list)\n",
    "        for pid, dist in self.topic_dist_arr.items():\n",
    "            for a in self.paper_to_authors.get(pid, []):\n",
    "                author_topic_acc[a].append(dist)\n",
    "        self.auth_topic_dict = {\n",
    "            a: np.mean(vs, axis=0)\n",
    "            for a, vs in author_topic_acc.items()\n",
    "        }\n",
    "        # per‐paper “domain” vector via its authors\n",
    "        self.paper_dom_dict = {\n",
    "            pid: np.mean(\n",
    "                [ self.auth_topic_dict[a] for a in authors if a in self.auth_topic_dict ],\n",
    "                axis=0\n",
    "            )\n",
    "            for pid, authors in self.paper_to_authors.items()\n",
    "            if any(a in self.auth_topic_dict for a in authors)\n",
    "        }\n",
    "        print(f\"INFO: BERTopic dicts ready (topics={len(self.topic_dist_arr)},\" \n",
    "              f\" authors={len(self.auth_topic_dict)}, papers_dom={len(self.paper_dom_dict)})\")\n",
    "        \n",
    "        \n",
    "        # ***Μενει Να φτιαξω απο τα δικα μου clean authors\n",
    "        with open(r\"D:\\NLP\\kozel\\embeddings\\author_emb.pkl\", \"rb\") as f:\n",
    "            self.emb_auth = pickle.load(f)   # dict: paper_id → 32‐d vector\n",
    "        print(f\"INFO: Loaded SVD embeddings (abstracts={len(self.emb_abs)}, authors={len(self.emb_auth)})\")\n",
    "        # ——————————————————————————————————————————————————————————————\n",
    "        \n",
    "        # j) Build citation graph (unweighted)\n",
    "        train_pos = self.df_pairs[(self.df_pairs.split==\"train\") & (self.df_pairs.label==1)]\n",
    "        self.G = nx.DiGraph()\n",
    "        self.G.add_nodes_from(\n",
    "            pd.unique(self.df_pairs[[\"citing\",\"cited\"]].values.ravel())\n",
    "        )\n",
    "        for u,v in zip(train_pos['citing'], train_pos['cited']):\n",
    "            self.G.add_edge(int(u), int(v))\n",
    "        print(f\"INFO: Built citation graph (nodes={self.G.number_of_nodes()}, edges={self.G.number_of_edges()})\")\n",
    "        \n",
    "        # h) Build co-author graph (weighted by # coauthored papers)\n",
    "        G_auth = nx.Graph()\n",
    "        for authors in self.paper_to_authors.values():\n",
    "            for a,b in combinations(authors,2):\n",
    "                if G_auth.has_edge(a,b):\n",
    "                    G_auth[a][b]['weight'] += 1\n",
    "                else:\n",
    "                    G_auth.add_edge(a,b,weight=1)\n",
    "        self.G_auth = G_auth\n",
    "        print(f\"INFO: Built co-author graph (nodes={G_auth.number_of_nodes()}, edges={G_auth.number_of_edges()})\")\n",
    "        \n",
    "        print(\"✅ Data loaded, graphs ready.\")\n",
    "        return self\n",
    "    \n",
    "    def _flush_feature(self, name, array):\n",
    "        \"\"\"Save feature array to disk and optionally free memory.\"\"\"\n",
    "        np.save(os.path.join(self.output_dir, f\"{name}.npy\"), array)\n",
    "        if self.chunked:\n",
    "            self.feat_dict.pop(name, None)\n",
    "        else:\n",
    "            self.feat_dict[name] = array\n",
    "        print(f\"    • Flushed feature '{name}' ({array.shape})\")\n",
    "    \n",
    "    def compute_tfidf_similarity(self, batch_size=10000):\n",
    "        \"\"\"\n",
    "        (Παλιό όνομα, αλλά πλέον κάνει cosine similarity\n",
    "         στα 32-διάστατα LSA embeddings αντί για raw TF–IDF.)\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Abstract-LSA sim ({n} pairs) est ~{n/self.its:.1f}s\")\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # προσωρινός πίνακας για τα αποτελέσματα\n",
    "        sim = np.zeros(n, dtype=float)\n",
    "    \n",
    "        # θα αντλήσουμε σειρές από self.emb_abs βάσει pid_to_idx\n",
    "        D = self.emb_abs.shape[1]\n",
    "        zero = np.zeros(D, dtype=float)\n",
    "    \n",
    "        u = self.df_pairs['citing'].to_numpy()\n",
    "        v = self.df_pairs['cited'].to_numpy()\n",
    "    \n",
    "        # batch loop για να μην γεμίσουμε μνήμη\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            for i in range(start, end):\n",
    "                pid_u, pid_v = u[i], v[i]\n",
    "                idx_u = self.pid_to_idx.get(pid_u, -1)\n",
    "                idx_v = self.pid_to_idx.get(pid_v, -1)\n",
    "    \n",
    "                eu = self.emb_abs[idx_u] if idx_u >= 0 else zero\n",
    "                ev = self.emb_abs[idx_v] if idx_v >= 0 else zero\n",
    "    \n",
    "                num = float(np.dot(eu, ev))\n",
    "                den = np.linalg.norm(eu) * np.linalg.norm(ev) + 1e-8\n",
    "                sim[i] = num / den\n",
    "    \n",
    "        # και το flush όπως πριν\n",
    "        self._flush_feature('tfidf_similarity', sim)\n",
    "        print(f\"INFO: Abstract-LSA sim done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    def compute_specter_similarity(self, batch_size=10000):\n",
    "        \"\"\"\n",
    "        Compute three Specter-based features for each (citing,cited):\n",
    "          • dot-product\n",
    "          • cosine similarity\n",
    "          • L1 distance\n",
    "        Flushes all three into 'specter_feats.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Specter feats ({n} pairs) est ~{n/self.its:.1f}s\")\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # prepare arrays\n",
    "        dots      = np.zeros(n, dtype=float)\n",
    "        cos_sims  = np.zeros(n, dtype=float)\n",
    "        abs_diffs = np.zeros(n, dtype=float)\n",
    "        specter_l2= np.zeros(n, dtype=float)\n",
    "    \n",
    "        # helper\n",
    "        D     = self.specter_emb.shape[1]\n",
    "        zero  = np.zeros(D, dtype=float)\n",
    "        pid2i = self.pid_to_idx  # paper→row in emb array\n",
    "    \n",
    "        u = self.df_pairs['citing'].to_numpy()\n",
    "        v = self.df_pairs['cited'].to_numpy()\n",
    "    \n",
    "        # batch-loop\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            for i in range(start, end):\n",
    "                pid_u, pid_v = u[i], v[i]\n",
    "                iu = pid2i.get(pid_u, -1)\n",
    "                iv = pid2i.get(pid_v, -1)\n",
    "                eu = self.specter_emb[iu] if iu >= 0 else zero\n",
    "                ev = self.specter_emb[iv] if iv >= 0 else zero\n",
    "    \n",
    "                d = float(np.dot(eu, ev))\n",
    "                dots[i] = d\n",
    "                # cosine\n",
    "                nu = np.linalg.norm(eu)\n",
    "                nv = np.linalg.norm(ev)\n",
    "                cos_sims[i] = d / (nu * nv + 1e-8)\n",
    "\n",
    "                diff = eu - ev\n",
    "                # L1\n",
    "                abs_diffs[i] = float(np.sum(np.abs(diff)))\n",
    "                # L2\n",
    "                specter_l2[i] =float(np.sum(diff * diff))\n",
    "    \n",
    "        # flush all three at once\n",
    "        out = os.path.join(self.output_dir, 'specter_feats.npz')\n",
    "        np.savez(\n",
    "            out,\n",
    "            specter_dot       = dots,\n",
    "            specter_cosine    = cos_sims,\n",
    "            specter_l1        = abs_diffs,\n",
    "            specter_l2        = specter_l2\n",
    "        )\n",
    "        print(f\"INFO: Specter feats done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "\n",
    "        \n",
    "    # def compute_scibert_similarity(self):\n",
    "    #     \"\"\"Compute cosine similarity using SciBERT embeddings.\"\"\"\n",
    "    #     # To be implemented\n",
    "    #     pass\n",
    "    def compute_scibert_similarity(self, batch_size=100000):\n",
    "        \"\"\"Compute and flush SciBERT cosine similarity in batches to save memory.\"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n/self.its\n",
    "        print(f\"INFO: SciBERT sim: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "        sim = np.zeros(n, float)\n",
    "        # dimension of SciBERT embeddings\n",
    "        D = len(next(iter(self.scibert_dict.values())))\n",
    "        # arrays of ids\n",
    "        u = self.df_pairs.citing.to_numpy(); v = self.df_pairs.cited.to_numpy()\n",
    "        # process in batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_batch = u[start:end]; v_batch = v[start:end]\n",
    "            # stack embeddings for this batch\n",
    "            U = np.vstack([self.scibert_dict.get(pid, np.zeros(D)) for pid in u_batch])\n",
    "            V = np.vstack([self.scibert_dict.get(pid, np.zeros(D)) for pid in v_batch])\n",
    "            dots = np.einsum('ij,ij->i', U, V)\n",
    "            nu = np.linalg.norm(U, axis=1); nv = np.linalg.norm(V, axis=1)\n",
    "            sim[start:end] = dots / (nu * nv + 1e-8)\n",
    "        # flush full feature\n",
    "        self._flush_feature('scibert_similarity', sim)\n",
    "        print(f\"INFO: SciBERT done in {time.time()-t0:.1f}s\")\n",
    "    \n",
    "    # def compute_bertopic_features(self):\n",
    "    #     pass\n",
    "    def compute_bertopic_features(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute BERTopic features for each (citing, cited) pair:\n",
    "          - citing & cited dominant topic\n",
    "          - same topic flag\n",
    "          - citing & cited topic entropy\n",
    "          - cosine similarity of full distributions\n",
    "        All 6 features are stacked into one array of shape (n_pairs, 6) \n",
    "        and flushed as a single file.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: BERTopic feats: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1. Load BERTopic table\n",
    "        df_bt = pd.read_parquet(self.bertopic_path)\n",
    "        dom_dict = dict(zip(df_bt.paper_id, df_bt.bertopic_dominant_topic))\n",
    "        ent_dict = dict(zip(df_bt.paper_id, df_bt.bertopic_topic_entropy))\n",
    "        dist_cols = [c for c in df_bt.columns if c.startswith(\"topic_dist_\")]\n",
    "        dist_mat = df_bt.set_index(\"paper_id\")[dist_cols]\n",
    "\n",
    "        # 2. Prepare id arrays\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # 3. Initialize arrays\n",
    "        citing_dom = np.array([dom_dict.get(pid, -1) for pid in u],   dtype=int)\n",
    "        cited_dom  = np.array([dom_dict.get(pid, -1) for pid in v],   dtype=int)\n",
    "        same_bt    = (citing_dom == cited_dom).astype(int)\n",
    "        citing_ent = np.array([ent_dict.get(pid, 0.0) for pid in u],  dtype=float)\n",
    "        cited_ent  = np.array([ent_dict.get(pid, 0.0) for pid in v],  dtype=float)\n",
    "        cos_sim    = np.zeros(n, dtype=float)\n",
    "\n",
    "        # 4. Cosine similarity in batches\n",
    "        D = len(dist_cols)\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_batch = u[start:end]\n",
    "            v_batch = v[start:end]\n",
    "\n",
    "            U = np.vstack([\n",
    "                dist_mat.loc[pid].to_numpy() if pid in dist_mat.index else np.zeros(D)\n",
    "                for pid in u_batch\n",
    "            ])\n",
    "            V = np.vstack([\n",
    "                dist_mat.loc[pid].to_numpy() if pid in dist_mat.index else np.zeros(D)\n",
    "                for pid in v_batch\n",
    "            ])\n",
    "\n",
    "            dots = np.einsum('ij,ij->i', U, V)\n",
    "            nu   = np.linalg.norm(U, axis=1)\n",
    "            nv   = np.linalg.norm(V, axis=1)\n",
    "            cos_sim[start:end] = dots / (nu * nv + 1e-8)\n",
    "\n",
    "        # 5. Stack all 6 features into one 2D array (n_pairs × 6)\n",
    "        all_feats = np.column_stack([\n",
    "            citing_dom,\n",
    "            cited_dom,\n",
    "            same_bt,\n",
    "            citing_ent,\n",
    "            cited_ent,\n",
    "            cos_sim\n",
    "        ])\n",
    "\n",
    "        # 6. Flush as a single file\n",
    "        self._flush_feature('bertopic_features', all_feats)\n",
    "        print(f\"INFO: BERTopic done in {time.time() - t0:.1f}s\")\n",
    "    \n",
    "    # def compute_lda_topics_features(self):\n",
    "    #     \"\"\"Compute similarity or distributions from LDA topics.\"\"\"\n",
    "    #     # To be implemented\n",
    "    #     pass\n",
    "    def compute_lda_topics_features(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between LDA topic distributions for each (citing, cited) pair in batches.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: LDA topics sim: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1. Prepare mapping from paper_id to topic vector\n",
    "        #    assume self.lda_topics has columns: \"paper_id\" + one column per topic\n",
    "        topic_cols = [c for c in self.lda_topics.columns if c != \"paper_id\"]\n",
    "        lda_mat = self.lda_topics.set_index(\"paper_id\")[topic_cols]\n",
    "        \n",
    "        # 2. Arrays of citing/cited IDs\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # 3. Preallocate result\n",
    "        sim = np.zeros(n, dtype=float)\n",
    "\n",
    "        # 4. Process in batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_batch = u[start:end]\n",
    "            v_batch = v[start:end]\n",
    "\n",
    "            # stack vectors, default to zero-vector if missing\n",
    "            U = np.vstack([\n",
    "                lda_mat.loc[pid].to_numpy() if pid in lda_mat.index else np.zeros(len(topic_cols))\n",
    "                for pid in u_batch\n",
    "            ])\n",
    "            V = np.vstack([\n",
    "                lda_mat.loc[pid].to_numpy() if pid in lda_mat.index else np.zeros(len(topic_cols))\n",
    "                for pid in v_batch\n",
    "            ])\n",
    "\n",
    "            # cosine similarity\n",
    "            dots = np.einsum('ij,ij->i', U, V)\n",
    "            nu = np.linalg.norm(U, axis=1)\n",
    "            nv = np.linalg.norm(V, axis=1)\n",
    "            sim[start:end] = dots / (nu * nv + 1e-8)\n",
    "\n",
    "        # 5. Flush feature array\n",
    "        self._flush_feature('lda_topics_similarity', sim)\n",
    "        print(f\"INFO: LDA topics done in {time.time()-t0:.1f}s\")\n",
    "    \n",
    "    # def compute_author_graph_heuristics(self):\n",
    "    #     pass\n",
    "    def compute_author_graph_heuristics(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute author-graph heuristics in batches:\n",
    "          - average common neighbors count per author-pair\n",
    "          - average Adamic–Adar per author-pair\n",
    "          - average Resource Allocation per author-pair\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Author-graph heuristics: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare ID arrays and result buffers\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        cn_arr = np.zeros(n, dtype=float)\n",
    "        aa_arr = np.zeros(n, dtype=float)\n",
    "        ra_arr = np.zeros(n, dtype=float)\n",
    "\n",
    "        # Local references for speed\n",
    "        G_auth = self.G_auth\n",
    "        deg_auth = dict(G_auth.degree())\n",
    "\n",
    "        # Process in batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_batch = u[start:end]\n",
    "            v_batch = v[start:end]\n",
    "\n",
    "            for i, (uid, vid) in enumerate(zip(u_batch, v_batch)):\n",
    "                # Get author lists for each paper\n",
    "                Au = self.paper_to_authors.get(uid, [])\n",
    "                Av = self.paper_to_authors.get(vid, [])\n",
    "                cn_vals, aa_vals, ra_vals = [], [], []\n",
    "\n",
    "                # Compute per-author-pair metrics\n",
    "                for a, b in product(Au, Av):\n",
    "                    if G_auth.has_node(a) and G_auth.has_node(b):\n",
    "                        common = list(nx.common_neighbors(G_auth, a, b))\n",
    "                        if common:\n",
    "                            cn_vals.append(len(common))\n",
    "                            aa_vals.append(sum(1.0 / np.log(1 + deg_auth[z]) for z in common))\n",
    "                            ra_vals.append(sum(1.0 / deg_auth[z] for z in common))\n",
    "\n",
    "                # Aggregate (mean) or default to 0\n",
    "                idx = start + i\n",
    "                if cn_vals:\n",
    "                    cn_arr[idx] = np.mean(cn_vals)\n",
    "                    aa_arr[idx] = np.mean(aa_vals)\n",
    "                    ra_arr[idx] = np.mean(ra_vals)\n",
    "\n",
    "        # Flush to disk\n",
    "        # Stack all three author-graph features into one array (n_pairs × 3)\n",
    "        all_feats = np.column_stack([\n",
    "            cn_arr,\n",
    "            aa_arr,\n",
    "            ra_arr\n",
    "        ])\n",
    "        # Flush as a single file\n",
    "        self._flush_feature('author_graph_heuristics', all_feats)\n",
    "        print(f\"INFO: Author-graph heuristics done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "    \n",
    "    # def compute_embedding_features(self):\n",
    "    #     pass\n",
    "    def compute_embedding_features(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute embedding-based scalars in batches:\n",
    "          - citation_node2vec_cosine, _dot, _l2\n",
    "          - citation_walklets_cosine, _dot, _l2\n",
    "          - author_node2vec_cosine, _dot, _l2\n",
    "          - author_walklets_cosine, _dot, _l2\n",
    "        Flushes all 12 arrays in a single .npz.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Embedding feats: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare id arrays\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # Allocate buffers\n",
    "        cnv_c_cos = np.zeros(n, dtype=float)\n",
    "        cnv_c_dot = np.zeros(n, dtype=float)\n",
    "        cnv_c_l2  = np.zeros(n, dtype=float)\n",
    "        cnv_w_cos = np.zeros(n, dtype=float)\n",
    "        cnv_w_dot = np.zeros(n, dtype=float)\n",
    "        cnv_w_l2  = np.zeros(n, dtype=float)\n",
    "        # Precompute author-level mean embeddings\n",
    "        # assume self.author_node2vec & self.author_walklets exist\n",
    "        # and self.paper_to_authors maps pid→list of a_ids\n",
    "        def mean_emb(pid, emb_dict, dim):\n",
    "            vs = [emb_dict[a] for a in self.paper_to_authors.get(pid, []) if a in emb_dict]\n",
    "            return np.mean(vs, axis=0) if vs else np.zeros(dim, dtype=float)\n",
    "        # determine dims\n",
    "        d_cn = next(iter(self.citation_node2vec.values())).shape[0]\n",
    "        d_aw = next(iter(self.citation_walklets.values())).shape[0]\n",
    "        d_an = next(iter(self.author_node2vec.values())).shape[0]\n",
    "        d_awl= next(iter(self.author_walklets.values())).shape[0]\n",
    "        # process in batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_b, v_b = u[start:end], v[start:end]\n",
    "            # citation-node2vec\n",
    "            Cv = np.vstack([ self.citation_node2vec.get(pid, np.zeros(d_cn)) for pid in u_b ])\n",
    "            Vv = np.vstack([ self.citation_node2vec.get(pid, np.zeros(d_cn)) for pid in v_b ])\n",
    "            dot = np.einsum('ij,ij->i', Cv, Vv)\n",
    "            nu  = np.linalg.norm(Cv, axis=1); nv = np.linalg.norm(Vv, axis=1)\n",
    "            cnv_c_cos[start:end] = dot / (nu*nv + 1e-8)\n",
    "            cnv_c_dot[start:end] = dot\n",
    "            cnv_c_l2[start:end]  = np.linalg.norm(Cv - Vv, axis=1)\n",
    "            # citation-walklets\n",
    "            Cw = np.vstack([ self.citation_walklets.get(pid, np.zeros(d_aw)) for pid in u_b ])\n",
    "            Wv = np.vstack([ self.citation_walklets.get(pid, np.zeros(d_aw)) for pid in v_b ])\n",
    "            dot = np.einsum('ij,ij->i', Cw, Wv)\n",
    "            nu  = np.linalg.norm(Cw, axis=1); nv = np.linalg.norm(Wv, axis=1)\n",
    "            cnv_w_cos[start:end] = dot / (nu*nv + 1e-8)\n",
    "            cnv_w_dot[start:end] = dot\n",
    "            cnv_w_l2[start:end]  = np.linalg.norm(Cw - Wv, axis=1)\n",
    "            # author-node2vec\n",
    "            An = np.vstack([ mean_emb(pid, self.author_node2vec, d_an) for pid in u_b ])\n",
    "            Bn = np.vstack([ mean_emb(pid, self.author_node2vec, d_an) for pid in v_b ])\n",
    "            dot = np.einsum('ij,ij->i', An, Bn)\n",
    "            nu  = np.linalg.norm(An, axis=1); nv = np.linalg.norm(Bn, axis=1)\n",
    "            # reuse buffers names for author if desired, or separate\n",
    "            # here stacking all into one npz with clear keys below\n",
    "            # similarly for author-walklets\n",
    "            Aw = np.vstack([ mean_emb(pid, self.author_walklets, d_awl) for pid in u_b ])\n",
    "            Bw = np.vstack([ mean_emb(pid, self.author_walklets, d_awl) for pid in v_b ])\n",
    "            dot_aw = np.einsum('ij,ij->i', Aw, Bw)\n",
    "            nu_aw  = np.linalg.norm(Aw, axis=1); nv_aw = np.linalg.norm(Bw, axis=1)\n",
    "            # store author embeddings\n",
    "            if start == 0:\n",
    "                an_cos = np.zeros(n, dtype=float)\n",
    "                an_dot = np.zeros(n, dtype=float)\n",
    "                an_l2  = np.zeros(n, dtype=float)\n",
    "                aw_cos = np.zeros(n, dtype=float)\n",
    "                aw_dot = np.zeros(n, dtype=float)\n",
    "                aw_l2  = np.zeros(n, dtype=float)\n",
    "            an_cos[start:end] = dot / (nu*nv + 1e-8)\n",
    "            an_dot[start:end] = dot\n",
    "            an_l2[start:end]  = np.linalg.norm(An - Bn, axis=1)\n",
    "            aw_cos[start:end] = dot_aw / (nu_aw*nv_aw + 1e-8)\n",
    "            aw_dot[start:end] = dot_aw\n",
    "            aw_l2[start:end]  = np.linalg.norm(Aw - Bw, axis=1)\n",
    "\n",
    "        # stack and flush all 12 features\n",
    "        np.savez(\n",
    "            os.path.join(self.output_dir, 'embedding_features.npz'),\n",
    "            citation_node2vec_cosine       = cnv_c_cos,\n",
    "            citation_node2vec_dot          = cnv_c_dot,\n",
    "            citation_node2vec_l2           = cnv_c_l2,\n",
    "            citation_walklets_cosine       = cnv_w_cos,\n",
    "            citation_walklets_dot          = cnv_w_dot,\n",
    "            citation_walklets_l2           = cnv_w_l2,\n",
    "            author_node2vec_cosine         = an_cos,\n",
    "            author_node2vec_dot            = an_dot,\n",
    "            author_node2vec_l2             = an_l2,\n",
    "            author_walklets_cosine         = aw_cos,\n",
    "            author_walklets_dot            = aw_dot,\n",
    "            author_walklets_l2             = aw_l2\n",
    "        )\n",
    "        print(f\"INFO: Embedding feats done in {time.time()-t0:.1f}s\")\n",
    "    \n",
    "    # def compute_coauthor_distance(self):\n",
    "    #     pass\n",
    "    def compute_coauthor_distance(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute co-author distance metrics in batches:\n",
    "          - coauth_min_dist: minimum shortest-path between any author of citing & cited\n",
    "          - coauth_mean_dist: average such shortest-path\n",
    "          - coauth_max_dist: maximum such shortest-path\n",
    "          - coauth_inv_min: 1 / (min_dist + 1)\n",
    "          - coauth_close_bin: binary flag if min_dist <= 2\n",
    "        Flushes all five arrays as a single .npz.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Co-author distance: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare id arrays and result buffers\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        min_arr  = np.zeros(n, dtype=float)\n",
    "        mean_arr = np.zeros(n, dtype=float)\n",
    "        max_arr  = np.zeros(n, dtype=float)\n",
    "\n",
    "        # Maximum distance if no path exists\n",
    "        max_dist = self.G_auth.number_of_nodes()\n",
    "\n",
    "        # Process in batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_batch = u[start:end]\n",
    "            v_batch = v[start:end]\n",
    "\n",
    "            for i, (uid, vid) in enumerate(zip(u_batch, v_batch)):\n",
    "                Au = self.paper_to_authors.get(uid, [])\n",
    "                Av = self.paper_to_authors.get(vid, [])\n",
    "                dists = []\n",
    "                for a in Au:\n",
    "                    for b in Av:\n",
    "                        if self.G_auth.has_node(a) and self.G_auth.has_node(b):\n",
    "                            try:\n",
    "                                dists.append(nx.shortest_path_length(self.G_auth, a, b))\n",
    "                            except nx.NetworkXNoPath:\n",
    "                                dists.append(max_dist)\n",
    "                idx = start + i\n",
    "                if dists:\n",
    "                    min_arr[idx]  = min(dists)\n",
    "                    mean_arr[idx] = sum(dists) / len(dists)\n",
    "                    max_arr[idx]  = max(dists)\n",
    "                else:\n",
    "                    min_arr[idx] = mean_arr[idx] = max_arr[idx] = max_dist\n",
    "\n",
    "        # Derived metrics\n",
    "        inv_min  = 1.0 / (min_arr + 1.0)\n",
    "        close_bin = (min_arr <= 2).astype(int)\n",
    "\n",
    "        # Flush all features together\n",
    "        out_path = os.path.join(self.output_dir, 'coauthor_distance.npz')\n",
    "        np.savez(\n",
    "            out_path,\n",
    "            coauth_min_dist  = min_arr,\n",
    "            coauth_mean_dist = mean_arr,\n",
    "            coauth_max_dist  = max_arr,\n",
    "            coauth_inv_min   = inv_min,\n",
    "            coauth_close_bin = close_bin\n",
    "        )\n",
    "        print(f\"INFO: Co-author distance done in {time.time() - t0:.1f}s, flushed to {out_path}\")\n",
    "\n",
    "    def compute_author_overlap_jaccard(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute author overlap and Jaccard coefficient for each pair:\n",
    "          - author_overlap: count of common authors\n",
    "          - jaccard_authors: |intersection| / |union|\n",
    "        Results saved together in 'author_overlap_jaccard.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Author overlap/Jaccard: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Precompute author sets\n",
    "        auth_sets = {pid: set(auths) for pid, auths in self.paper_to_authors.items()}\n",
    "\n",
    "        # Prepare buffers\n",
    "        overlap = np.zeros(n, dtype=int)\n",
    "        jaccard = np.zeros(n, dtype=float)\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # Batch processing\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            for i, (uid, vid) in enumerate(zip(u[start:end], v[start:end])):\n",
    "                Au = auth_sets.get(uid, set())\n",
    "                Av = auth_sets.get(vid, set())\n",
    "                inter = Au & Av\n",
    "                uni   = Au | Av\n",
    "                idx = start + i\n",
    "                overlap[idx] = len(inter)\n",
    "                jaccard[idx] = len(inter) / (len(uni) + 1e-8)\n",
    "\n",
    "        # Flush both features in one .npz\n",
    "        out_path = os.path.join(self.output_dir, 'author_overlap_jaccard.npz')\n",
    "        np.savez(\n",
    "            out_path,\n",
    "            author_overlap   = overlap,\n",
    "            jaccard_authors  = jaccard\n",
    "        )\n",
    "        print(f\"INFO: Author overlap/Jaccard done in {time.time() - t0:.1f}s, flushed to {out_path}\")\n",
    "\n",
    "\n",
    "    def compute_author_aggregate_stats(self):\n",
    "        \"\"\"\n",
    "        Compute author-level aggregate stats for each pair:\n",
    "          - citing_author_mean_pagerank, cited_author_mean_pagerank\n",
    "          - citing_author_max_pagerank,  cited_author_max_pagerank\n",
    "          - citing_author_mean_degree,   cited_author_mean_degree\n",
    "        Results saved together in 'author_aggregate_stats.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Author agg stats: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Compute pagerank and degree on co-author graph\n",
    "        auth_pr  = nx.pagerank(self.G_auth, weight='weight')\n",
    "        auth_deg = dict(self.G_auth.degree())\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        mean_pr_u = np.zeros(n, dtype=float)\n",
    "        mean_pr_v = np.zeros(n, dtype=float)\n",
    "        max_pr_u  = np.zeros(n, dtype=float)\n",
    "        max_pr_v  = np.zeros(n, dtype=float)\n",
    "        mean_deg_u = np.zeros(n, dtype=float)\n",
    "        mean_deg_v = np.zeros(n, dtype=float)\n",
    "\n",
    "        for idx, (uid, vid) in enumerate(zip(u, v)):\n",
    "            Au = self.paper_to_authors.get(uid, [])\n",
    "            Av = self.paper_to_authors.get(vid, [])\n",
    "            # pagerank stats\n",
    "            prs_u = [auth_pr.get(a, 0.0) for a in Au]\n",
    "            prs_v = [auth_pr.get(a, 0.0) for a in Av]\n",
    "            if prs_u:\n",
    "                mean_pr_u[idx] = sum(prs_u) / len(prs_u)\n",
    "                max_pr_u[idx]  = max(prs_u)\n",
    "            if prs_v:\n",
    "                mean_pr_v[idx] = sum(prs_v) / len(prs_v)\n",
    "                max_pr_v[idx]  = max(prs_v)\n",
    "            # degree stats\n",
    "            degs_u = [auth_deg.get(a, 0) for a in Au]\n",
    "            degs_v = [auth_deg.get(a, 0) for a in Av]\n",
    "            if degs_u:\n",
    "                mean_deg_u[idx] = sum(degs_u) / len(degs_u)\n",
    "            if degs_v:\n",
    "                mean_deg_v[idx] = sum(degs_v) / len(degs_v)\n",
    "\n",
    "        # Flush all six stats together\n",
    "        out_path = os.path.join(self.output_dir, 'author_aggregate_stats.npz')\n",
    "        np.savez(\n",
    "            out_path,\n",
    "            citing_author_mean_pagerank = mean_pr_u,\n",
    "            cited_author_mean_pagerank  = mean_pr_v,\n",
    "            citing_author_max_pagerank  = max_pr_u,\n",
    "            cited_author_max_pagerank   = max_pr_v,\n",
    "            citing_author_mean_degree   = mean_deg_u,\n",
    "            cited_author_mean_degree    = mean_deg_v\n",
    "        )\n",
    "        print(f\"INFO: Author agg stats done in {time.time() - t0:.1f}s, flushed to {out_path}\")\n",
    "    def compute_node_level_metrics(self):\n",
    "        \"\"\"\n",
    "        Compute node‐level graph features for each (citing, cited) pair:\n",
    "          - citing_in_degree, citing_out_degree, citing_degree\n",
    "          - cited_in_degree,  cited_out_degree,  cited_degree\n",
    "          - citing_pagerank,   cited_pagerank\n",
    "          - citing_triangles,  cited_triangles\n",
    "          - citing_clustering, cited_clustering\n",
    "          - citing_core,       cited_core\n",
    "          - citing_onion,      cited_onion\n",
    "          - citing_eigen,      cited_eigen\n",
    "          - common_neighbors (undirected)\n",
    "        Flush all 21 arrays as one .npz.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Node‐level metrics: {n} pairs (~{n/self.its:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Build undirected version\n",
    "        und = self.G.to_undirected()\n",
    "\n",
    "        # Compute raw dicts\n",
    "        in_deg   = dict(self.G.in_degree())\n",
    "        out_deg  = dict(self.G.out_degree())\n",
    "        deg      = dict(self.G.degree())\n",
    "        pr       = nx.pagerank(self.G, weight=None)\n",
    "        tri      = nx.triangles(und)\n",
    "        clust    = nx.clustering(und, weight=None)\n",
    "        core     = nx.core_number(und)\n",
    "        onion    = nx.onion_layers(und)\n",
    "        eig      = nx.eigenvector_centrality(self.G, max_iter=500)\n",
    "\n",
    "        # Prepare index arrays\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # Map into arrays\n",
    "        ci_deg = np.array([in_deg.get(x,0) for x in u])\n",
    "        co_deg = np.array([out_deg.get(x,0) for x in u])\n",
    "        ct_deg = np.array([deg.get(x,0) for x in u])\n",
    "        di_deg = np.array([in_deg.get(x,0) for x in v])\n",
    "        do_deg = np.array([out_deg.get(x,0) for x in v])\n",
    "        dt_deg = np.array([deg.get(x,0) for x in v])\n",
    "\n",
    "        ci_pr = np.array([pr.get(x,0.0) for x in u])\n",
    "        co_pr = np.array([pr.get(x,0.0) for x in v])\n",
    "\n",
    "        ci_tri = np.array([tri.get(x,0) for x in u])\n",
    "        co_tri = np.array([tri.get(x,0) for x in v])\n",
    "\n",
    "        ci_cl = np.array([clust.get(x,0.0) for x in u])\n",
    "        co_cl = np.array([clust.get(x,0.0) for x in v])\n",
    "\n",
    "        ci_co = np.array([core.get(x,0) for x in u])\n",
    "        co_co = np.array([core.get(x,0) for x in v])\n",
    "\n",
    "        ci_on = np.array([onion.get(x,0) for x in u])\n",
    "        co_on = np.array([onion.get(x,0) for x in v])\n",
    "\n",
    "        ci_ei = np.array([eig.get(x,0.0) for x in u])\n",
    "        co_ei = np.array([eig.get(x,0.0) for x in v])\n",
    "\n",
    "        # Common neighbors via adjacency-squared\n",
    "        nodes = list(und.nodes())\n",
    "        idx_map = {node:i for i,node in enumerate(nodes)}\n",
    "        A = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A)\n",
    "        ui = [idx_map[x] for x in u]\n",
    "        vi = [idx_map[x] for x in v]\n",
    "        cn = np.array(A2[ui, vi]).ravel()\n",
    "\n",
    "        # Stack and flush\n",
    "        np.savez(\n",
    "            os.path.join(self.output_dir, 'node_level_metrics.npz'),\n",
    "            citing_in_degree       = ci_deg,\n",
    "            citing_out_degree      = co_deg,\n",
    "            citing_degree          = ct_deg,\n",
    "            cited_in_degree        = di_deg,\n",
    "            cited_out_degree       = do_deg,\n",
    "            cited_degree           = dt_deg,\n",
    "            citing_pagerank        = ci_pr,\n",
    "            cited_pagerank         = co_pr,\n",
    "            citing_triangles       = ci_tri,\n",
    "            cited_triangles        = co_tri,\n",
    "            citing_clustering      = ci_cl,\n",
    "            cited_clustering       = co_cl,\n",
    "            citing_core_number     = ci_co,\n",
    "            cited_core_number      = co_co,\n",
    "            citing_onion_number    = ci_on,\n",
    "            cited_onion_number     = co_on,\n",
    "            citing_eigenvector     = ci_ei,\n",
    "            cited_eigenvector      = co_ei,\n",
    "            common_neighbors       = cn\n",
    "        )\n",
    "        print(f\"INFO: Node‐level done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "\n",
    "    def compute_pair_heuristics(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute pair‐level heuristics on the citation graph:\n",
    "          - citation_jaccard, salton, hub_depressed, adamic_adar\n",
    "          - preferential_attachment, resource_allocation\n",
    "          - directed_shortest_path (–1 if none)\n",
    "        Flush all 7 arrays in 'pair_heuristics.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Pair heuristics: {n} pairs (~{n/self.its:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        und = self.G.to_undirected()\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # Precompute deg and common_neighbors via A2\n",
    "        deg_dict = dict(self.G.degree())\n",
    "        ui = u; vi = v\n",
    "        nodes = list(und.nodes())\n",
    "        idx_map = {node:i for i,node in enumerate(nodes)}\n",
    "        A = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A)\n",
    "        cn = np.array([A2[idx_map[x], idx_map[y]] for x,y in zip(u,v)])\n",
    "\n",
    "        # Buffers\n",
    "        jacc = cn / (np.array([deg_dict.get(x,0) + deg_dict.get(y,0) - c for x,y,c in zip(u,v,cn)]) + 1e-8)\n",
    "        sal   = cn / np.sqrt(np.array([deg_dict.get(x,0)*deg_dict.get(y,0) for x,y in zip(u,v)]) + 1e-8)\n",
    "        hub   = cn / (np.maximum([deg_dict.get(x,0) for x in u],[deg_dict.get(y,0) for y in v]) + 1e-8)\n",
    "        # Adamic–Adar & RA\n",
    "        aa = np.zeros(n, dtype=float)\n",
    "        ra = np.zeros(n, dtype=float)\n",
    "        for i,(x,y) in enumerate(zip(u,v)):\n",
    "            aa[i] = sum(1.0/np.log(1+und.degree(z)) for z in nx.common_neighbors(und, x, y))\n",
    "            ra[i] = sum(1.0/und.degree(z)        for z in nx.common_neighbors(und, x, y))\n",
    "        # Preferential attachment\n",
    "        pa = np.array([self.G.out_degree(x)*self.G.in_degree(y) for x,y in zip(u,v)])\n",
    "        # Directed shortest paths (batch)\n",
    "        dsp = np.full(n, -1, dtype=int)\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start+batch_size, n)\n",
    "            for i,(x,y) in enumerate(zip(u[start:end],v[start:end])):\n",
    "                try:\n",
    "                    dsp[start+i] = nx.shortest_path_length(self.G, x, y)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    dsp[start+i] = -1\n",
    "\n",
    "        \n",
    "\n",
    "        # Flush\n",
    "        np.savez(\n",
    "            os.path.join(self.output_dir,'pair_heuristics.npz'),\n",
    "            citation_G_jaccard              = jacc,\n",
    "            citation_G_salton               = sal,\n",
    "            citation_G_hub_depressed        = hub,\n",
    "            citation_G_adamic_adar          = aa,\n",
    "            citation_G_resource_allocation  = ra,\n",
    "            citation_G_preferential_attachment = pa,\n",
    "            citation_G_sp_directed          = dsp\n",
    "        )\n",
    "        print(f\"INFO: Pair heuristics done in {time.time()-t0:.1f}s\")\n",
    "    \n",
    "\n",
    "    def compute_co_citation_bibliographic(self):\n",
    "        \"\"\"\n",
    "        Compute co-citation and bibliographic coupling counts:\n",
    "          - co_citation: |predecessors(u) ∩ predecessors(v)|\n",
    "          - bibliographic_coupling: |successors(u) ∩ successors(v)|\n",
    "        Flushes both arrays in 'cocite_biblio.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Co-citation & bibliographic: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        G = self.G\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        co_cite = np.zeros(n, dtype=int)\n",
    "        biblio  = np.zeros(n, dtype=int)\n",
    "\n",
    "        for i, (uid, vid) in enumerate(zip(u, v)):\n",
    "            preds_u = set(G.predecessors(uid))\n",
    "            preds_v = set(G.predecessors(vid))\n",
    "            co_cite[i] = len(preds_u & preds_v)\n",
    "\n",
    "            succs_u = set(G.successors(uid))\n",
    "            succs_v = set(G.successors(vid))\n",
    "            biblio[i] = len(succs_u & succs_v)\n",
    "\n",
    "        out_path = os.path.join(self.output_dir, 'cocite_biblio.npz')\n",
    "        np.savez(\n",
    "            out_path,\n",
    "            co_citation              = co_cite,\n",
    "            bibliographic_coupling   = biblio\n",
    "        )\n",
    "        print(f\"INFO: Co-citation & biblio done in {time.time()-t0:.1f}s, flushed to {out_path}\")\n",
    "\n",
    "    def compute_path_based_scores(self, beta=0.005, epsilon=0.001):\n",
    "        \"\"\"\n",
    "        Katz (χωρίς direct‐term), Local‐Path, και approx directed‐SP:\n",
    "          • katz_index = β²·A2 + β³·A3\n",
    "          • local_path = A2 + ε·A3\n",
    "          • sp_directed = 2 if A2>0, 3 if A3>0, else -1\n",
    "        \"\"\"\n",
    "        import scipy.sparse as sps\n",
    "    \n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Path-based scores: {n} pairs\")\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # 1) adjacency powers on undirected G\n",
    "        und = self.G.to_undirected()\n",
    "        nodes = list(und.nodes())\n",
    "        idx = {node:i for i,node in enumerate(nodes)}\n",
    "        A  = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A)\n",
    "        A3 = A2.dot(A)\n",
    "    \n",
    "        # 2) prepare output arrays\n",
    "        katz = np.zeros(n, dtype=float)\n",
    "        lp   = np.zeros(n, dtype=float)\n",
    "        spd  = np.full(n, -1,   dtype=int)\n",
    "    \n",
    "        u_ids = self.df_pairs['citing'].to_numpy()\n",
    "        v_ids = self.df_pairs['cited'].to_numpy()\n",
    "    \n",
    "        # 3) extract per-pair features\n",
    "        for i,(u,v) in enumerate(zip(u_ids, v_ids)):\n",
    "            ui, vi = idx[u], idx[v]\n",
    "            # κλασικό Katz χωρίς direct term\n",
    "            katz[i] = (beta**2) * A2[ui,vi] + (beta**3) * A3[ui,vi]\n",
    "            # Local-Path\n",
    "            lp[i]   = A2[ui,vi] + epsilon * A3[ui,vi]\n",
    "            # approx directed‐SP\n",
    "            if A2[ui,vi] > 0:\n",
    "                spd[i] = 2\n",
    "            elif A3[ui,vi] > 0:\n",
    "                spd[i] = 3\n",
    "            else:\n",
    "                spd[i] = -1\n",
    "    \n",
    "        # 4) flush all together\n",
    "        out = os.path.join(self.output_dir, 'path_based_scores.npz')\n",
    "        np.savez(\n",
    "            out,\n",
    "            katz_index = katz,\n",
    "            local_path = lp,\n",
    "            citation_G_sp_directed = spd\n",
    "        )\n",
    "        print(f\"INFO: Path-based done in {time.time()-t0:.1f}s → {out}\")\n",
    "\n",
    "\n",
    "    def compute_community_features(self):\n",
    "        \"\"\"\n",
    "        3. Community‐Based Features on citation graph:\n",
    "          • same_community: 1 if both in same Louvain community\n",
    "          • comm_size_ratio: |C_u| / |C_v|\n",
    "        Flushes both arrays in 'community_features.npz'.\n",
    "        \"\"\"\n",
    "        print(\"INFO: Detecting communities (greedy modularity)...\")\n",
    "        t0 = time.time()\n",
    "        und = self.G.to_undirected()\n",
    "        comms = nx.algorithms.community.greedy_modularity_communities(und)\n",
    "        comm_map = {node:cid for cid,comm in enumerate(comms) for node in comm}\n",
    "        sizes = {cid: len(comm) for cid,comm in enumerate(comms)}\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        n = len(u)\n",
    "        same  = np.zeros(n, dtype=int)\n",
    "        ratio = np.zeros(n, dtype=float)\n",
    "\n",
    "        for i,(x,y) in enumerate(zip(u,v)):\n",
    "            cx, cy = comm_map.get(x,-1), comm_map.get(y,-1)\n",
    "            same[i] = int(cx==cy and cx!=-1)\n",
    "            if cx in sizes and cy in sizes and sizes[cy]>0:\n",
    "                ratio[i] = sizes[cx] / sizes[cy]\n",
    "\n",
    "        out = os.path.join(self.output_dir, 'community_features.npz')\n",
    "        np.savez(out, same_community=same, comm_size_ratio=ratio)\n",
    "        print(f\"INFO: Community done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "\n",
    "    def compute_motif_counts(self):\n",
    "        \"\"\"\n",
    "        4. Higher‐Order Motif Counts:\n",
    "          • triangles_through_edge = A2[x,y]//2\n",
    "          • cycles4 ≈ number of paths length‐3 = A3[x,y]\n",
    "        Flushes both arrays in 'motif_counts.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Motif counts: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        und = self.G.to_undirected()\n",
    "        nodes = list(und.nodes()); idx = {node:i for i,node in enumerate(nodes)}\n",
    "        A  = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A); A3 = A2.dot(A)\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        tri  = np.zeros(n, dtype=int)\n",
    "        cyc4 = np.zeros(n, dtype=float)\n",
    "\n",
    "        for i,(x,y) in enumerate(zip(u,v)):\n",
    "            xi, yi = idx[x], idx[y]\n",
    "            tri[i]  = int(A2[xi,yi] // 2)\n",
    "            cyc4[i] = A3[xi,yi]\n",
    "\n",
    "        out = os.path.join(self.output_dir, 'motif_counts.npz')\n",
    "        np.savez(out, triangles=tri, cycles4=cyc4)\n",
    "        print(f\"INFO: Motifs done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "   \n",
    "    def extract_keywords_from_abstracts(self, abstracts_path, top_k=10, max_features=5000):\n",
    "        \"\"\"\n",
    "        5a. Extract top-k keywords per paper from abstracts via TF–IDF.\n",
    "        Stores self.paper_keywords: {paper_id: [kw1,…,kw_topk]}.\n",
    "        \"\"\"\n",
    "        print(\"INFO: Extracting keywords via TF–IDF from abstracts...\")\n",
    "        t0 = time.time()\n",
    "        # load abstracts parquet with columns ['paper_id','abstract']\n",
    "        df_abs = pd.read_parquet(abstracts_path)\n",
    "        texts  = df_abs['abstract'].fillna(\"\").tolist()\n",
    "        pids   = df_abs['paper_id'].tolist()\n",
    "\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vec = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        X = vec.fit_transform(texts)  # shape = (n_papers, max_features)\n",
    "        features = vec.get_feature_names_out()\n",
    "\n",
    "        self.paper_keywords = {}\n",
    "        for i, pid in enumerate(pids):\n",
    "            row = X[i].tocoo()\n",
    "            if row.nnz:\n",
    "                top_idx = np.argsort(row.data)[-top_k:]\n",
    "                kws = [features[row.col[j]] for j in top_idx]\n",
    "            else:\n",
    "                kws = []\n",
    "            self.paper_keywords[pid] = kws\n",
    "\n",
    "        print(f\"INFO: Keywords extracted for {len(pids)} papers in {time.time()-t0:.1f}s\")\n",
    "\n",
    "\n",
    "    def compute_content_overlap(self):\n",
    "        \"\"\"\n",
    "        5b. Content Overlap Beyond Abstracts:\n",
    "          • title_jaccard: (if titles exist)\n",
    "          • keyword_jaccard: Jaccard of extracted keywords\n",
    "        Requires run of extract_keywords_from_abstracts(...) first.\n",
    "        Flushes 'content_overlap.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Content overlap: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # assume self.paper_keywords exists\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        kw_j = np.zeros(n, dtype=float)\n",
    "\n",
    "        for i,(x,y) in enumerate(zip(u,v)):\n",
    "            Au = set(self.paper_keywords.get(x, []))\n",
    "            Av = set(self.paper_keywords.get(y, []))\n",
    "            inter = Au & Av\n",
    "            uni   = Au | Av\n",
    "            kw_j[i] = len(inter) / (len(uni) + 1e-8)\n",
    "\n",
    "        out = os.path.join(self.output_dir, 'content_overlap.npz')\n",
    "        np.savez(out, keyword_jaccard=kw_j)\n",
    "        print(f\"INFO: Content done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "    def compute_author_domain_similarity(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute author-domain similarity features:\n",
    "          • max_citing_topic_author_dom_cosine\n",
    "          • mean_citing_topic_author_dom_cosine\n",
    "          • min_citing_topic_author_dom_cosine\n",
    "          • max_cited_topic_author_dom_cosine\n",
    "          • mean_cited_topic_author_dom_cosine\n",
    "          • min_cited_topic_author_dom_cosine\n",
    "        Flushes all 6 arrays in 'author_domain_similarity.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Author-domain sim: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # prepare default zero‐vector for missing topics\n",
    "        D_topic = next(iter(self.topic_dist_arr.values())).shape[0]\n",
    "        zero_vec = np.zeros(D_topic, dtype=float)\n",
    "\n",
    "        # result buffers\n",
    "        max_u = np.zeros(n, float)\n",
    "        mean_u = np.zeros(n, float)\n",
    "        min_u = np.zeros(n, float)\n",
    "        max_v = np.zeros(n, float)\n",
    "        mean_v = np.zeros(n, float)\n",
    "        min_v = np.zeros(n, float)\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # batch over pairs\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            for i, (uid, vid) in enumerate(zip(u[start:end], v[start:end])):\n",
    "                idx = start + i\n",
    "\n",
    "                # domain vector for cited (used by citing→authors)\n",
    "                dom_v = self.topic_dist_arr.get(vid, zero_vec)\n",
    "                sims_u = []\n",
    "                for a in self.paper_to_authors.get(uid, []):\n",
    "                    vec_a = self.auth_topic_dict.get(a)\n",
    "                    if vec_a is not None:\n",
    "                        sims_u.append(\n",
    "                            float(\n",
    "                                np.dot(vec_a, dom_v) /\n",
    "                                (np.linalg.norm(vec_a) * np.linalg.norm(dom_v) + 1e-8)\n",
    "                            )\n",
    "                        )\n",
    "                if sims_u:\n",
    "                    max_u[idx], mean_u[idx], min_u[idx] = max(sims_u), np.mean(sims_u), min(sims_u)\n",
    "\n",
    "                # domain vector for citing (used by cited→authors)\n",
    "                dom_u = self.topic_dist_arr.get(uid, zero_vec)\n",
    "                sims_v = []\n",
    "                for a in self.paper_to_authors.get(vid, []):\n",
    "                    vec_a = self.auth_topic_dict.get(a)\n",
    "                    if vec_a is not None:\n",
    "                        sims_v.append(\n",
    "                            float(\n",
    "                                np.dot(vec_a, dom_u) /\n",
    "                                (np.linalg.norm(vec_a) * np.linalg.norm(dom_u) + 1e-8)\n",
    "                            )\n",
    "                        )\n",
    "                if sims_v:\n",
    "                    max_v[idx], mean_v[idx], min_v[idx] = max(sims_v), np.mean(sims_v), min(sims_v)\n",
    "\n",
    "        # flush to disk\n",
    "        out = os.path.join(self.output_dir, 'author_domain_similarity.npz')\n",
    "        np.savez(\n",
    "            out,\n",
    "            max_citing_topic_author_dom_cosine   = max_u,\n",
    "            mean_citing_topic_author_dom_cosine  = mean_u,\n",
    "            min_citing_topic_author_dom_cosine   = min_u,\n",
    "            max_cited_topic_author_dom_cosine    = max_v,\n",
    "            mean_cited_topic_author_dom_cosine   = mean_v,\n",
    "            min_cited_topic_author_dom_cosine    = min_v\n",
    "        )\n",
    "        print(f\"INFO: Author-domain done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "        \n",
    "    def compute_paper_author_domain_similarity(self):\n",
    "        \"\"\"\n",
    "        Compute paper‐level author‐domain similarity:\n",
    "          • paper_total_auth_dom_cosine_citing_vs_cited\n",
    "          • paper_total_auth_dom_dot_citing_vs_cited\n",
    "          • paper_total_auth_dom_l2_citing_vs_cited\n",
    "        Uses self.paper_dom_dict.\n",
    "        Flushes 3 arrays in 'paper_author_domain.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Paper–author‐domain sim: {n} pairs (~{n/self.its:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # build matrices\n",
    "        D = next(iter(self.paper_dom_dict.values())).shape[0]\n",
    "        U = np.vstack([ self.paper_dom_dict.get(pid, np.zeros(D)) for pid in u ])\n",
    "        V = np.vstack([ self.paper_dom_dict.get(pid, np.zeros(D)) for pid in v ])\n",
    "\n",
    "        dot = np.einsum('ij,ij->i', U, V)\n",
    "        nu  = np.linalg.norm(U, axis=1)\n",
    "        nv  = np.linalg.norm(V, axis=1)\n",
    "        cos = dot / (nu * nv + 1e-8)\n",
    "        l2  = np.linalg.norm(U - V, axis=1)\n",
    "\n",
    "        out = os.path.join(self.output_dir, 'paper_author_domain.npz')\n",
    "        np.savez(\n",
    "            out,\n",
    "            paper_total_auth_dom_cosine_citing_vs_cited = cos,\n",
    "            paper_total_auth_dom_dot_citing_vs_cited    = dot,\n",
    "            paper_total_auth_dom_l2_citing_vs_cited     = l2\n",
    "        )\n",
    "        print(f\"INFO: Paper–author‐domain done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "        \n",
    "\n",
    "    def compute_abstract_author_svd_similarity(self):\n",
    "        n = len(self.df_pairs)\n",
    "        dot_abs = np.zeros(n); cos_abs = np.zeros(n)\n",
    "        dot_auth= np.zeros(n); cos_auth= np.zeros(n)\n",
    "    \n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "    \n",
    "        for i,(uid,vid) in enumerate(zip(u,v)):\n",
    "            idx_u = self.pid_to_idx.get(uid, -1)\n",
    "            idx_v = self.pid_to_idx.get(vid, -1)\n",
    "            eu = self.emb_abs[idx_u] if idx_u>=0 else np.zeros(self.emb_abs.shape[1])\n",
    "            ev = self.emb_abs[idx_v] if idx_v>=0 else np.zeros(self.emb_abs.shape[1])\n",
    "            da = eu.dot(ev)\n",
    "            na = np.linalg.norm(eu); nb = np.linalg.norm(ev)\n",
    "            dot_abs[i] = da\n",
    "            cos_abs[i] = da/(na*nb+1e-8)\n",
    "    \n",
    "            # αν έχεις emb_auth\n",
    "            au = self.emb_auth.get(uid, np.zeros_like(eu))\n",
    "            av = self.emb_auth.get(vid, np.zeros_like(eu))\n",
    "            du = au.dot(av)\n",
    "            nu = np.linalg.norm(au); nv = np.linalg.norm(av)\n",
    "            dot_auth[i] = du\n",
    "            cos_auth[i] = du/(nu*nv+1e-8)\n",
    "    \n",
    "        out = os.path.join(self.output_dir,'svd_text_author.npz')\n",
    "        np.savez(out,\n",
    "                 cosine_abs_svd_koz  = cos_abs,\n",
    "                 cosine_auth_svd_koz = cos_auth)\n",
    "        print(f\"INFO: SVD text/author done.\")\n",
    "\n",
    "        \n",
    "    \n",
    "    def compute_rooted_pagerank(self, alpha=0.85, max_iter=100, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Compute Personalized (Rooted) PageRank score from each citing u to cited v,\n",
    "        grouping by u to avoid OOM and redundant computation:\n",
    "          • For each unique u, run PageRank on self.G with teleport vector focused on u\n",
    "          • Record the PageRank score at v for every (u,v) pair\n",
    "        Flushes array 'rooted_pagerank_score.npy'.\n",
    "        \"\"\"\n",
    "        n   = len(self.df_pairs)\n",
    "        # Estimate time assuming each PR is ~10× heavier than a single it/sec\n",
    "        est = n / (self.its / 10)\n",
    "        print(f\"INFO: Rooted PageRank: {n} pairs est ~{est:.1f}s\")\n",
    "        t0  = time.time()\n",
    "\n",
    "        # Prepare output buffer\n",
    "        scores = np.zeros(n, dtype=float)\n",
    "\n",
    "        # Group indices by citing‐paper u\n",
    "        from collections import defaultdict\n",
    "        idxs_by_u = defaultdict(list)\n",
    "        for idx, (u, v) in enumerate(zip(self.df_pairs.citing, self.df_pairs.cited)):\n",
    "            idxs_by_u[u].append((idx, v))\n",
    "\n",
    "        # Compute PageRank once per unique u, assign and free memory immediately\n",
    "        for u, lst in idxs_by_u.items():\n",
    "            # Build personalization vector: teleport only to u\n",
    "            pers = {node: 0.0 for node in self.G.nodes()}\n",
    "            pers[u] = 1.0\n",
    "            pr_u = nx.pagerank(\n",
    "                G=self.G,\n",
    "                alpha=alpha,\n",
    "                personalization=pers,\n",
    "                max_iter=max_iter,\n",
    "                tol=tol,\n",
    "                weight=None\n",
    "            )\n",
    "            # Assign scores for all (idx, v) belonging to this u\n",
    "            for idx, v in lst:\n",
    "                scores[idx] = pr_u.get(v, 0.0)\n",
    "            # Free the PageRank vector before next u\n",
    "            del pr_u\n",
    "\n",
    "        # Flush to disk\n",
    "        out = os.path.join(self.output_dir, 'rooted_pagerank_score.npy')\n",
    "        np.save(out, scores)\n",
    "        print(f\"INFO: Rooted PageRank done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "        \n",
    "    def compute_motif_and_path_features(self, beta=0.005, epsilon=0.001, batch_size=100_000):\n",
    "        \"\"\"\n",
    "        Memory‐safe, chunked computation of:\n",
    "          • katz_index            = β²·A² + β³·A³\n",
    "          • local_path            = A² + ε·A³\n",
    "          • triangles_through_edge= A²[u,v] // 2\n",
    "          • cycles4               = A³[u,v]\n",
    "          • citation_G_sp_directed= 2 if A²>0, 3 if A³>0, else -1\n",
    "    \n",
    "        Uses np.memmap to avoid holding all features in RAM,\n",
    "        and processes the pair list in chunks.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import time\n",
    "        import numpy as np\n",
    "        from numpy.lib.format import open_memmap\n",
    "    \n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Motif+Path feats (chunked): {n} pairs\")\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # — Build undirected adjacency and its powers once —\n",
    "        und = self.G.to_undirected()\n",
    "        nodes = list(und.nodes())\n",
    "        idx_map = {node: i for i, node in enumerate(nodes)}\n",
    "        A  = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A)\n",
    "        A3 = A2.dot(A)\n",
    "    \n",
    "        # — Prepare on‐disk arrays via memmap —\n",
    "        out_dir = self.output_dir\n",
    "        katz_mmap = open_memmap(os.path.join(out_dir, 'katz_index_h2.npy'),\n",
    "                            mode='w+', dtype='float32', shape=(n,))\n",
    "        lp_mmap   = open_memmap(os.path.join(out_dir, 'local_path_h2.npy'),\n",
    "                                mode='w+', dtype='float32', shape=(n,))\n",
    "        tri_mmap  = open_memmap(os.path.join(out_dir, 'triangles_through_edge_h2.npy'),\n",
    "                                mode='w+', dtype='int32',   shape=(n,))\n",
    "        cyc4_mmap = open_memmap(os.path.join(out_dir, 'cycles4_h2.npy'),\n",
    "                                mode='w+', dtype='int32',   shape=(n,))\n",
    "        spd_mmap  = open_memmap(os.path.join(out_dir, 'citation_G_sp_directed_h2.npy'),\n",
    "                                mode='w+', dtype='int8',    shape=(n,))\n",
    "      \n",
    "        \n",
    "    \n",
    "        # — Load u/v arrays once —\n",
    "        u_arr = self.df_pairs['citing'].to_numpy()\n",
    "        v_arr = self.df_pairs['cited'].to_numpy()\n",
    "    \n",
    "        # — Process in chunks —\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(n, start + batch_size)\n",
    "            for i in range(start, end):\n",
    "                u, v = u_arr[i], v_arr[i]\n",
    "                ui, vi = idx_map[u], idx_map[v]\n",
    "                a2 = A2[ui, vi]\n",
    "                a3 = A3[ui, vi]\n",
    "    \n",
    "                katz_mmap[i] = (beta**2) * a2 + (beta**3) * a3\n",
    "                lp_mmap[i]   = a2 + epsilon * a3\n",
    "                tri_mmap[i]  = int(a2 // 2)\n",
    "                cyc4_mmap[i] = int(a3)\n",
    "                if a2 > 0:\n",
    "                    spd_mmap[i] = 2\n",
    "                elif a3 > 0:\n",
    "                    spd_mmap[i] = 3\n",
    "                # else leaves -1\n",
    "    \n",
    "            print(f\"  • Processed rows {start}–{end} in {time.time()-t0:.1f}s\")\n",
    "    \n",
    "        # — flush & cleanup memmaps —\n",
    "        del katz_mmap, lp_mmap, tri_mmap, cyc4_mmap, spd_mmap\n",
    "\n",
    "        print(f\"INFO: Motif+Path done in {time.time()-t0:.1f}s \")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def compute_katz_centrality(self, beta=0.005, max_iter=1000, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Υπολογίζει Katz centrality (node‐level) με networkx.katz_centrality\n",
    "        για μεγάλο, αραιό γράφο self.G και αποθηκεύει ως NumPy array\n",
    "        το χαρακτηριστικό katz_centrality.npy για κάθε (citing, cited) ζεύγος.\n",
    "        Επιστρέφει το array.\n",
    "        \n",
    "        Παράμετροι:\n",
    "          - beta:     ο συντελεστής ύφεσης (alpha στο NetworkX)\n",
    "          - max_iter: μέγιστος αριθμός επαναλήψεων\n",
    "          - tol:      ανοχή σύγκλισης\n",
    "        \"\"\"\n",
    "        # 1) Katz centrality ανά κόμβο (iterative, sparse-friendly)\n",
    "        katz_scores = nx.katz_centrality(\n",
    "            self.G,\n",
    "            alpha=beta,\n",
    "            beta=1.0,\n",
    "            max_iter=max_iter,\n",
    "            tol=tol,\n",
    "            normalized=True\n",
    "        )\n",
    "\n",
    "        # 2) Δημιουργία array κεντρικοτήτων για κάθε ζεύγος (citing)\n",
    "        n = len(self.df_pairs)\n",
    "        katz_centrality = np.zeros(n, dtype=float)\n",
    "        for i, u in enumerate(self.df_pairs['citing'].astype(int)):\n",
    "            katz_centrality[i] = katz_scores.get(u, 0.0)\n",
    "\n",
    "        # 3) Αποθήκευση σε .npy\n",
    "        out_path = os.path.join(self.output_dir, 'katz_centrality.npy')\n",
    "        np.save(out_path, katz_centrality)\n",
    "        print(f\"INFO: Saved katz_centrality to {out_path}\")\n",
    "\n",
    "        return katz_centrality\n",
    "\n",
    "    ############################################################## EMBEDDINGS FEATURES #########################################################\n",
    "    def compute_specter_hadamard(self):\n",
    "        \"\"\"\n",
    "        Για κάθε (citing, cited) ζεύγος στο train split υπολογίζει\n",
    "        το element-wise Hadamard product των Specter embeddings\n",
    "        και το αποθηκεύει σε NumPy array shape = (n_train_pairs, D).\n",
    "        Επιστρέφει το array.\n",
    "        \"\"\"\n",
    "        # 1) Φόρτωση Specter embeddings\n",
    "        embeds = np.load(self.abstracts_emb_path)  # shape = [n_papers, D]\n",
    "        D = embeds.shape[1]\n",
    "\n",
    "        # 2) Επιλογή μόνο των train ζευγών\n",
    "        train_df = self.df_pairs[self.df_pairs['split'] == 'train']\n",
    "        u_ids = train_df['citing'].astype(int).to_numpy()\n",
    "        v_ids = train_df['cited'].astype(int).to_numpy()\n",
    "        n = len(train_df)\n",
    "\n",
    "        # 3) Πρίζουμε πίνακα για τα Hadamard features\n",
    "        hadamard = np.zeros((n, D), dtype=float)\n",
    "\n",
    "        # 4) Υπολογισμός element-wise product για κάθε ζεύγος\n",
    "        for i, (u, v) in enumerate(zip(u_ids, v_ids)):\n",
    "            hadamard[i, :] = embeds[u] * embeds[v]\n",
    "\n",
    "        # 5) Αποθήκευση σε .npy\n",
    "        out_path = os.path.join(self.output_dir, 'specter_hadamard.npy')\n",
    "        np.save(out_path, hadamard)\n",
    "        print(f\"INFO: Saved Specter Hadamard features to {out_path} (shape={hadamard.shape})\")\n",
    "\n",
    "        return hadamard\n",
    "\n",
    "        \n",
    "    def extract_all(self):\n",
    "        \"\"\"\n",
    "        Run all feature computation methods in sequence,\n",
    "        assemble into DataFrame, and return.\n",
    "        \"\"\"\n",
    "        print(\"Starting feature extraction...\")\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # sequence of feature methods\n",
    "        self.compute_tfidf_similarity()\n",
    "        self.compute_specter_similarity()\n",
    "        self.compute_scibert_similarity()\n",
    "        self.compute_bertopic_features()\n",
    "        self.compute_lda_topics_features()\n",
    "        self.compute_author_graph_heuristics()\n",
    "        self.compute_embedding_features()\n",
    "        self.compute_coauthor_distance()\n",
    "        self.compute_temporal_features()\n",
    "        \n",
    "        # assemble DataFrame\n",
    "        df_feat = pd.DataFrame(self.feat_dict, index=self.df_pairs.index)\n",
    "        result = pd.concat([self.df_pairs.reset_index(drop=True), df_feat], axis=1)\n",
    "        \n",
    "        print(f\"✅ All features extracted in {time.time() - t0:.2f}s\")\n",
    "        return result\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e37000-68f6-47e9-ad32-75737a7b8c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c882961-4b69-47d0-9c62-4e599f17bbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Loading data...\n",
      "INFO: Loaded 2183910 pairs in 0.7s\n",
      "INFO: Loaded authors mapping (138499 papers)\n",
      "INFO: Loaded abstracts_embeds (138499, 300)\n",
      "INFO: Loaded pid_to_idx (138499 entries)\n",
      "INFO: Loaded Specter embeddings array (138499, 768)\n",
      "INFO: Loaded SciBERT embeddings (138499)\n",
      "INFO: Loaded LDA topics (131250) in 1.8s total\n",
      "INFO: Loaded graph embeddings: citation_node2vec=138499, citation_walklets=138499, author_node2vec=136863, author_walklets=136863\n",
      "INFO: BERTopic dicts ready (topics=131250, authors=133493, papers_dom=138192)\n",
      "INFO: Loaded SVD embeddings (abstracts=138499, authors=138499)\n",
      "INFO: Built citation graph (nodes=138499, edges=982760)\n",
      "INFO: Built co-author graph (nodes=136863, edges=520007)\n",
      "✅ Data loaded, graphs ready.\n",
      "INFO: Specter feats (2183910 pairs) est ~10.9s\n",
      "INFO: Specter feats done in 50.4s, flushed to D:/NLP/Features_XL/train\\specter_feats.npz\n",
      "INFO: Saved Specter Hadamard features to D:/NLP/Features_XL/train\\specter_hadamard.npy (shape=(1965519, 300))\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"D:/NLP/tfidf_xgboost\"\n",
    "    output_dir = \"D:/NLP/Features_XL/train\"\n",
    "    fe = FeatureExtractor(base_path, output_dir, chunked=True, its=200000)\n",
    "    fe.load_data()  # loads both train/val or test as needed\n",
    "   # fe.compute_tfidf_similarity()\n",
    "    #fe.compute_specter_similarity()\n",
    "    #fe.compute_scibert_similarity()\n",
    "    #fe.compute_bertopic_features()\n",
    "    #fe.compute_lda_topics_features()\n",
    "    #fe.compute_author_graph_heuristics()\n",
    "    #fe.compute_coauthor_distance()\n",
    "    #fe.compute_author_overlap_jaccard()\n",
    "    #fe.compute_author_aggregate_stats()\n",
    "    #fe.compute_node_level_metrics()\n",
    "    \n",
    "    #fe.compute_pair_heuristics() # takes long\n",
    "   \n",
    "    #fe.compute_co_citation_bibliographic()\n",
    "    \n",
    "    #fe.compute_path_based_scores() \n",
    "    #fe.compute_community_features() # takes long #INFO: Community done in 34467.4s, = 10 hours flushed to D:/NLP/Features_XL/train\\community_features.npz\n",
    "\n",
    "    #fe.compute_motif_counts()\n",
    "    #fe.extract_keywords_from_abstracts(\"D:/NLP/tfidf_xgboost/cleaned_abstracts.parquet\", top_k=10)\n",
    "    #fe.compute_content_overlap()\n",
    "\n",
    "\n",
    "\n",
    "    #fe.compute_author_domain_similarity()\n",
    "    #fe.compute_paper_author_domain_similarity()\n",
    "  #  fe.compute_abstract_author_svd_similarity()\n",
    "\n",
    "    #fe.compute_embedding_features()\n",
    "    \n",
    "    #fe.compute_rooted_pagerank()\n",
    "    \n",
    "    #fe.compute_motif_and_path_features()\n",
    "    #fe.compute_katz_centrality()\n",
    "\n",
    "\n",
    "    ##################################################### EMBEDDING FEATURES ########################################################################\n",
    "    fe.compute_specter_hadamard()\n",
    "    # ... and so on for other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22b0e12-e011-41ec-bf92-615de3ef2f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citing</th>\n",
       "      <th>cited</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>tfidf_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58335</td>\n",
       "      <td>113748</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.100756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18144</td>\n",
       "      <td>90516</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.005954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87793</td>\n",
       "      <td>130708</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.018084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33290</td>\n",
       "      <td>12998</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.018639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77459</td>\n",
       "      <td>19749</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.014315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   citing   cited  label  split  tfidf_similarity\n",
       "0   58335  113748      0  train          0.100756\n",
       "1   18144   90516      0  train          0.005954\n",
       "2   87793  130708      0  train          0.018084\n",
       "3   33290   12998      1  train          0.018639\n",
       "4   77459   19749      0  train          0.014315"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997277e-97a5-4187-9742-7b50a80cbb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING FEATURES EXTRACTION\n",
    "# SPECTER HADAMARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f13bc-e1d6-49e8-abbb-241b5161a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "961a940a-0122-4d64-b272-af9e02a84313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from itertools import combinations, product\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "class TestFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract features for citation link‐prediction on an unseen test set.\n",
    "    Reads a txt file of (citing,cited) pairs with no header, and computes\n",
    "    the same features as the training extractor.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_path, output_dir, chunked=True, its=200000):\n",
    "        # same resource paths as training\n",
    "        self.base_path        = base_path\n",
    "        self.authors_path     = os.path.join(base_path, \"paper_to_authors.pkl\")\n",
    "        \n",
    "        self.abstracts_emb_path = r\"D:/NLP/citation_link_prediction/abstracts_embeds.npy\"\n",
    "        self.tfidf_idx_path = f\"{base_path}/tfidf_pid_to_idx.pkl\"\n",
    "        \n",
    "        self.specter_path = r'D:\\NLP\\citation_link_prediction\\specter_pretrained.npy'\n",
    "        self.scibert_path = \"D:/NLP/data/paper_scibert_embeddings.pkl\"\n",
    "        self.bertopic_path    = os.path.join(base_path, \"bertopic_features.parquet\")\n",
    "        self.lda_topics_path  = os.path.join(base_path, \"paper_topics.parquet\")\n",
    "        \n",
    "        # output\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.chunked = chunked\n",
    "        self.its     = its\n",
    "\n",
    "        # placeholders\n",
    "        self.df_pairs       = None\n",
    "        self.paper_to_authors = None\n",
    "        self.pid_to_idx     = None\n",
    "        self.tfidf_matrix   = None\n",
    "        self.specter_dict   = None\n",
    "\n",
    "        self.specter_emb  = None\n",
    "        \n",
    "        self.scibert_dict = None\n",
    "        self.lda_topics     = None\n",
    "        self.topic_dict     = None\n",
    "        self.entropy_dict   = None\n",
    "        self.topic_dist_arr = None\n",
    "        self.auth_topic_dict= None\n",
    "        self.paper_dom_dict = None\n",
    "        self.G              = None\n",
    "        self.G_auth         = None\n",
    "        \n",
    "        # will hold in-memory features if not chunked\n",
    "        self.feat_dict = {}  \n",
    "\n",
    "    def load_data(self, test_pairs_path):\n",
    "        \"\"\"\n",
    "        Load test pairs (headerless txt), plus all mappings & embeddings.\n",
    "        Creates dummy split/label columns so compute_* methods run unchanged.\n",
    "        \"\"\"\n",
    "        print(\"INFO: Loading test data…\")\n",
    "        t0 = time.time()\n",
    "        # -- 1. Load test pairs --\n",
    "        self.df_pairs = pd.read_csv(\n",
    "            test_pairs_path,\n",
    "            header=None,\n",
    "            names=[\"citing\",\"cited\"],\n",
    "            dtype={\"citing\":int,\"cited\":int}\n",
    "        )\n",
    "        # dummy columns\n",
    "        self.df_pairs[\"split\"] = \"test\"\n",
    "        self.df_pairs[\"label\"] = -1\n",
    "        print(f\"INFO: Loaded {len(self.df_pairs)} test pairs in {time.time()-t0:.1f}s\")\n",
    "\n",
    "         # b) Paper→authors mapping\n",
    "        with open(self.authors_path, \"rb\") as f:\n",
    "            self.paper_to_authors = pickle.load(f)\n",
    "        print(f\"INFO: Loaded authors mapping ({len(self.paper_to_authors)} papers)\")\n",
    "\n",
    "        # c) TF–IDF index & matrix\n",
    "        # ► i) Load precomputed SVD embeddings for abstracts & authors\n",
    "        #(My tfidf instead of Kozel)\n",
    "        # load abstracts SVD embeddings (LSA 32d)\n",
    "        self.emb_abs = np.load(self.abstracts_emb_path)\n",
    "        # emb_abs.shape == (n_rows_in_tfidf_matrix, 32)\n",
    "        print(f\"INFO: Loaded abstracts_embeds {self.emb_abs.shape}\")\n",
    "\n",
    "        # μέσα στο load_data(), μετά το \"LOAD TF-IDF index\"\n",
    "        with open(self.tfidf_idx_path, \"rb\") as f:\n",
    "            self.pid_to_idx = pickle.load(f)\n",
    "        print(f\"INFO: Loaded pid_to_idx ({len(self.pid_to_idx)} entries)\")\n",
    "      \n",
    "        \n",
    "        # d) SPECTER embeddings\n",
    "        # Load Specter embeddings from .npy (shape = [n_papers_used, D_s])\n",
    "        self.specter_emb = np.load(self.specter_path)\n",
    "        print(f\"INFO: Loaded Specter embeddings array {self.specter_emb.shape}\")\n",
    "        \n",
    "        # e) SciBERT embeddings\n",
    "        with open(self.scibert_path, \"rb\") as f:\n",
    "            self.scibert_dict = pickle.load(f)\n",
    "        print(f\"INFO: Loaded SciBERT embeddings ({len(self.scibert_dict)})\")\n",
    "        \n",
    "        # f) BERTopic & LDA topics\n",
    "        self.lda_topics = pd.read_parquet(self.lda_topics_path)\n",
    "        print(f\"INFO: Loaded LDA topics ({len(self.lda_topics)}) in {time.time()-t0:.1f}s total\")\n",
    "\n",
    "\n",
    "        ## EMBEDS\n",
    "        with open(os.path.join(self.base_path,\n",
    "                               \"split_train_val\",\n",
    "                               \"citation_node2vec_tuned.pkl\"), \"rb\") as f:\n",
    "            self.citation_node2vec = pickle.load(f)\n",
    "        with open(os.path.join(self.base_path,\n",
    "                               \"split_train_val\",\n",
    "                               \"citation_node2vec_directed_weighted_q2.pkl\"), \"rb\") as f:\n",
    "            self.citation_walklets = pickle.load(f)\n",
    "        with open(os.path.join(self.base_path, \"author_node2vec.pkl\"), \"rb\") as f:\n",
    "            self.author_node2vec = pickle.load(f)\n",
    "        with open(os.path.join(self.base_path, \"author_walklets.pkl\"), \"rb\") as f:\n",
    "            self.author_walklets = pickle.load(f)\n",
    "        print(f\"INFO: Loaded graph embeddings: \"\n",
    "              f\"citation_node2vec={len(self.citation_node2vec)}, \"\n",
    "              f\"citation_walklets={len(self.citation_walklets)}, \"\n",
    "              f\"author_node2vec={len(self.author_node2vec)}, \"\n",
    "              f\"author_walklets={len(self.author_walklets)}\")\n",
    "\n",
    "        # ——————————————————————————————————————————————————————————————\n",
    "        # ► h) Load BERTopic features and build topic‐dicts once for all functions\n",
    "        df_bt = pd.read_parquet(self.bertopic_path)\n",
    "        # dominant topic & entropy\n",
    "        self.topic_dict   = dict(zip(df_bt.paper_id, df_bt.bertopic_dominant_topic))\n",
    "        self.entropy_dict = dict(zip(df_bt.paper_id, df_bt.bertopic_topic_entropy))\n",
    "        # full distribution vectors\n",
    "        topic_cols = [c for c in df_bt.columns if c.startswith(\"topic_dist_\")]\n",
    "        td_df = df_bt.set_index(\"paper_id\")[topic_cols]\n",
    "        self.topic_dist_arr = {\n",
    "            pid: td_df.loc[pid].to_numpy()\n",
    "            for pid in td_df.index\n",
    "        }\n",
    "        # per‐author average topic vector\n",
    "        from collections import defaultdict\n",
    "        author_topic_acc = defaultdict(list)\n",
    "        for pid, dist in self.topic_dist_arr.items():\n",
    "            for a in self.paper_to_authors.get(pid, []):\n",
    "                author_topic_acc[a].append(dist)\n",
    "        self.auth_topic_dict = {\n",
    "            a: np.mean(vs, axis=0)\n",
    "            for a, vs in author_topic_acc.items()\n",
    "        }\n",
    "        # per‐paper “domain” vector via its authors\n",
    "        self.paper_dom_dict = {\n",
    "            pid: np.mean(\n",
    "                [ self.auth_topic_dict[a] for a in authors if a in self.auth_topic_dict ],\n",
    "                axis=0\n",
    "            )\n",
    "            for pid, authors in self.paper_to_authors.items()\n",
    "            if any(a in self.auth_topic_dict for a in authors)\n",
    "        }\n",
    "        print(f\"INFO: BERTopic dicts ready (topics={len(self.topic_dist_arr)},\" \n",
    "              f\" authors={len(self.auth_topic_dict)}, papers_dom={len(self.paper_dom_dict)})\")\n",
    "        \n",
    "\n",
    "       # Kozel Authors SVD\n",
    "        with open(r\"D:\\NLP\\kozel\\embeddings\\author_emb.pkl\", \"rb\") as f:\n",
    "            self.emb_auth = pickle.load(f)   # dict: paper_id → 32‐d vector\n",
    "        print(f\"INFO: Loaded SVD embeddings (abstracts={len(self.emb_abs)}, authors={len(self.emb_auth)})\")\n",
    "        # ——————————————————————————————————————————————————————————————\n",
    "        \n",
    "        # j) Build citation graph (unweighted)\n",
    "        train_pos = self.df_pairs[(self.df_pairs.split==\"train\") & (self.df_pairs.label==1)]\n",
    "        self.G = nx.DiGraph()\n",
    "        self.G.add_nodes_from(\n",
    "            pd.unique(self.df_pairs[[\"citing\",\"cited\"]].values.ravel())\n",
    "        )\n",
    "        for u,v in zip(train_pos['citing'], train_pos['cited']):\n",
    "            self.G.add_edge(int(u), int(v))\n",
    "        print(f\"INFO: Built citation graph (nodes={self.G.number_of_nodes()}, edges={self.G.number_of_edges()})\")\n",
    "        \n",
    "        # h) Build co-author graph (weighted by # coauthored papers)\n",
    "        G_auth = nx.Graph()\n",
    "        for authors in self.paper_to_authors.values():\n",
    "            for a,b in combinations(authors,2):\n",
    "                if G_auth.has_edge(a,b):\n",
    "                    G_auth[a][b]['weight'] += 1\n",
    "                else:\n",
    "                    G_auth.add_edge(a,b,weight=1)\n",
    "        self.G_auth = G_auth\n",
    "        print(f\"INFO: Built co-author graph (nodes={G_auth.number_of_nodes()}, edges={G_auth.number_of_edges()})\")\n",
    "        \n",
    "        print(\"✅ Data loaded, graphs ready.\")\n",
    "        return self\n",
    "\n",
    "    def _flush(self, name, arr):\n",
    "        np.save(os.path.join(self.output_dir, f\"{name}.npy\"), arr)\n",
    "        print(f\"    • Flushed {name} ({arr.shape})\")\n",
    "\n",
    "    # --- example compute methods, same signatures as train class ---\n",
    "    def compute_tfidf_similarity(self, batch_size=10000):\n",
    "        \"\"\"\n",
    "        (Παλιό όνομα, αλλά πλέον κάνει cosine similarity\n",
    "         στα 32-διάστατα LSA embeddings αντί για raw TF–IDF.)\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Abstract-LSA sim ({n} pairs) est ~{n/self.its:.1f}s\")\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # προσωρινός πίνακας για τα αποτελέσματα\n",
    "        sim = np.zeros(n, dtype=float)\n",
    "    \n",
    "        # θα αντλήσουμε σειρές από self.emb_abs βάσει pid_to_idx\n",
    "        D = self.emb_abs.shape[1]\n",
    "        zero = np.zeros(D, dtype=float)\n",
    "    \n",
    "        u = self.df_pairs['citing'].to_numpy()\n",
    "        v = self.df_pairs['cited'].to_numpy()\n",
    "    \n",
    "        # batch loop για να μην γεμίσουμε μνήμη\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            for i in range(start, end):\n",
    "                pid_u, pid_v = u[i], v[i]\n",
    "                idx_u = self.pid_to_idx.get(pid_u, -1)\n",
    "                idx_v = self.pid_to_idx.get(pid_v, -1)\n",
    "    \n",
    "                eu = self.emb_abs[idx_u] if idx_u >= 0 else zero\n",
    "                ev = self.emb_abs[idx_v] if idx_v >= 0 else zero\n",
    "    \n",
    "                num = float(np.dot(eu, ev))\n",
    "                den = np.linalg.norm(eu) * np.linalg.norm(ev) + 1e-8\n",
    "                sim[i] = num / den\n",
    "    \n",
    "        # και το flush όπως πριν\n",
    "        self._flush('tfidf_similarity', sim)\n",
    "        print(f\"INFO: Abstract-LSA sim done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    def compute_specter_similarity(self, batch_size=10000):\n",
    "        \"\"\"\n",
    "        Compute three Specter-based features for each (citing,cited):\n",
    "          • dot-product\n",
    "          • cosine similarity\n",
    "          • L1 distance\n",
    "          • L2 distance\n",
    "          \n",
    "        Flushes all three into 'specter_feats.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Specter feats ({n} pairs) est ~{n/self.its:.1f}s\")\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # prepare arrays\n",
    "        dots      = np.zeros(n, dtype=float)\n",
    "        cos_sims  = np.zeros(n, dtype=float)\n",
    "        abs_diffs = np.zeros(n, dtype=float)\n",
    "        specter_l2= np.zeros(n, dtype=float)\n",
    "    \n",
    "        # helper\n",
    "        D     = self.specter_emb.shape[1]\n",
    "        zero  = np.zeros(D, dtype=float)\n",
    "        pid2i = self.pid_to_idx  # paper→row in emb array\n",
    "    \n",
    "        u = self.df_pairs['citing'].to_numpy()\n",
    "        v = self.df_pairs['cited'].to_numpy()\n",
    "    \n",
    "        # batch-loop\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            for i in range(start, end):\n",
    "                pid_u, pid_v = u[i], v[i]\n",
    "                iu = pid2i.get(pid_u, -1)\n",
    "                iv = pid2i.get(pid_v, -1)\n",
    "                eu = self.specter_emb[iu] if iu >= 0 else zero\n",
    "                ev = self.specter_emb[iv] if iv >= 0 else zero\n",
    "    \n",
    "                d = float(np.dot(eu, ev))\n",
    "                dots[i] = d\n",
    "                # cosine\n",
    "                nu = np.linalg.norm(eu)\n",
    "                nv = np.linalg.norm(ev)\n",
    "                cos_sims[i] = d / (nu * nv + 1e-8)\n",
    "\n",
    "                diff = eu - ev\n",
    "                # L1\n",
    "                abs_diffs[i] = float(np.sum(np.abs(diff)))\n",
    "                # L2\n",
    "                specter_l2[i] =float(np.sum(diff * diff))\n",
    "    \n",
    "        # flush all three at once\n",
    "        out = os.path.join(self.output_dir, 'specter_feats.npz')\n",
    "        np.savez(\n",
    "            out,\n",
    "            specter_dot       = dots,\n",
    "            specter_cosine    = cos_sims,\n",
    "            specter_l1        = abs_diffs,\n",
    "            specter_l2        = specter_l2\n",
    "        )\n",
    "        print(f\"INFO: Specter feats done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "\n",
    "    def compute_scibert_similarity(self, batch_size=100000):\n",
    "        \"\"\"Compute and flush SciBERT cosine similarity in batches to save memory.\"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n/self.its\n",
    "        print(f\"INFO: SciBERT sim: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "        sim = np.zeros(n, float)\n",
    "        # dimension of SciBERT embeddings\n",
    "        D = len(next(iter(self.scibert_dict.values())))\n",
    "        # arrays of ids\n",
    "        u = self.df_pairs.citing.to_numpy(); v = self.df_pairs.cited.to_numpy()\n",
    "        # process in batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_batch = u[start:end]; v_batch = v[start:end]\n",
    "            # stack embeddings for this batch\n",
    "            U = np.vstack([self.scibert_dict.get(pid, np.zeros(D)) for pid in u_batch])\n",
    "            V = np.vstack([self.scibert_dict.get(pid, np.zeros(D)) for pid in v_batch])\n",
    "            dots = np.einsum('ij,ij->i', U, V)\n",
    "            nu = np.linalg.norm(U, axis=1); nv = np.linalg.norm(V, axis=1)\n",
    "            sim[start:end] = dots / (nu * nv + 1e-8)\n",
    "        # flush full feature\n",
    "        self._flush('scibert_similarity', sim)\n",
    "        print(f\"INFO: SciBERT done in {time.time()-t0:.1f}s\")\n",
    "    \n",
    "    # def compute_bertopic_features(self):\n",
    "    #     pass\n",
    "    def compute_bertopic_features(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute BERTopic features for each (citing, cited) pair:\n",
    "          - citing & cited dominant topic\n",
    "          - same topic flag\n",
    "          - citing & cited topic entropy\n",
    "          - cosine similarity of full distributions\n",
    "        All 6 features are stacked into one array of shape (n_pairs, 6) \n",
    "        and flushed as a single file.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: BERTopic feats: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # 1. Load BERTopic table\n",
    "        df_bt = pd.read_parquet(self.bertopic_path)\n",
    "        dom_dict = dict(zip(df_bt.paper_id, df_bt.bertopic_dominant_topic))\n",
    "        ent_dict = dict(zip(df_bt.paper_id, df_bt.bertopic_topic_entropy))\n",
    "        dist_cols = [c for c in df_bt.columns if c.startswith(\"topic_dist_\")]\n",
    "        dist_mat = df_bt.set_index(\"paper_id\")[dist_cols]\n",
    "\n",
    "        # 2. Prepare id arrays\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # 3. Initialize arrays\n",
    "        citing_dom = np.array([dom_dict.get(pid, -1) for pid in u],   dtype=int)\n",
    "        cited_dom  = np.array([dom_dict.get(pid, -1) for pid in v],   dtype=int)\n",
    "        same_bt    = (citing_dom == cited_dom).astype(int)\n",
    "        citing_ent = np.array([ent_dict.get(pid, 0.0) for pid in u],  dtype=float)\n",
    "        cited_ent  = np.array([ent_dict.get(pid, 0.0) for pid in v],  dtype=float)\n",
    "        cos_sim    = np.zeros(n, dtype=float)\n",
    "\n",
    "        # 4. Cosine similarity in batches\n",
    "        D = len(dist_cols)\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_batch = u[start:end]\n",
    "            v_batch = v[start:end]\n",
    "\n",
    "            U = np.vstack([\n",
    "                dist_mat.loc[pid].to_numpy() if pid in dist_mat.index else np.zeros(D)\n",
    "                for pid in u_batch\n",
    "            ])\n",
    "            V = np.vstack([\n",
    "                dist_mat.loc[pid].to_numpy() if pid in dist_mat.index else np.zeros(D)\n",
    "                for pid in v_batch\n",
    "            ])\n",
    "\n",
    "            dots = np.einsum('ij,ij->i', U, V)\n",
    "            nu   = np.linalg.norm(U, axis=1)\n",
    "            nv   = np.linalg.norm(V, axis=1)\n",
    "            cos_sim[start:end] = dots / (nu * nv + 1e-8)\n",
    "\n",
    "        # 5. Stack all 6 features into one 2D array (n_pairs × 6)\n",
    "        all_feats = np.column_stack([\n",
    "            citing_dom,\n",
    "            cited_dom,\n",
    "            same_bt,\n",
    "            citing_ent,\n",
    "            cited_ent,\n",
    "            cos_sim\n",
    "        ])\n",
    "\n",
    "        # 6. Flush as a single file\n",
    "        self._flush('bertopic_features', all_feats)\n",
    "        print(f\"INFO: BERTopic done in {time.time() - t0:.1f}s\")\n",
    "    \n",
    "\n",
    "    # … υλοποιήστε ανάλογα compute_… για:\n",
    "    #    compute_co_citation_bibliographic,\n",
    "    #    compute_author_overlap_jaccard,\n",
    "    #    compute_pair_heuristics,\n",
    "    #    compute_lda_topics_features,\n",
    "    #    compute_community_features,\n",
    "    #    compute_content_overlap,\n",
    "    #    compute_node_level_metrics\n",
    "    # με ακριβώς την ίδια λογική όπως στη FeatureExtractor κλάση.\n",
    "    def compute_co_citation_bibliographic(self):\n",
    "        \"\"\"\n",
    "        Co‐citation & bibliographic coupling counts:\n",
    "          • co_citation      = |pred(u) ∩ pred(v)|\n",
    "          • bibliographic_coupling = |succ(u) ∩ succ(v)|\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        co_cite = np.zeros(n, int)\n",
    "        biblio  = np.zeros(n, int)\n",
    "        for i, (u, v) in enumerate(zip(self.df_pairs.citing, self.df_pairs.cited)):\n",
    "            preds_u = set(self.G.predecessors(u))\n",
    "            preds_v = set(self.G.predecessors(v))\n",
    "            co_cite[i] = len(preds_u & preds_v)\n",
    "            succs_u = set(self.G.successors(u))\n",
    "            succs_v = set(self.G.successors(v))\n",
    "            biblio[i] = len(succs_u & succs_v)\n",
    "        self._flush('co_citation', co_cite)\n",
    "        self._flush('bibliographic_coupling', biblio)\n",
    "\n",
    "    def compute_author_overlap_jaccard(self):\n",
    "        \"\"\"\n",
    "        Author overlap & Jaccard per paper‐pair:\n",
    "          • author_overlap = |Authors(u) ∩ Authors(v)|\n",
    "          • jaccard_authors = |Au ∩ Av| / |Au ∪ Av|\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        overlap = np.zeros(n, int)\n",
    "        jaccard = np.zeros(n, float)\n",
    "        for i, (u, v) in enumerate(zip(self.df_pairs.citing, self.df_pairs.cited)):\n",
    "            Au = set(self.paper_to_authors.get(u, []))\n",
    "            Av = set(self.paper_to_authors.get(v, []))\n",
    "            inter = Au & Av\n",
    "            uni   = Au | Av\n",
    "            overlap[i] = len(inter)\n",
    "            jaccard[i] = len(inter) / (len(uni) + 1e-8)\n",
    "        self._flush('author_overlap', overlap)\n",
    "        self._flush('jaccard_authors', jaccard)\n",
    "\n",
    "\n",
    "    def compute_lda_topics_features(self):\n",
    "        \"\"\"\n",
    "        Cosine similarity of LDA topic distributions per paper‐pair.\n",
    "        \"\"\"\n",
    "        topic_cols = [c for c in self.lda_topics.columns if c!='paper_id']\n",
    "        lda_mat = self.lda_topics.set_index('paper_id')[topic_cols]\n",
    "        n = len(self.df_pairs)\n",
    "        sim = np.zeros(n, float)\n",
    "        for i, (u, v) in enumerate(zip(self.df_pairs.citing, self.df_pairs.cited)):\n",
    "            a = lda_mat.loc[u].to_numpy() if u in lda_mat.index else np.zeros(len(topic_cols))\n",
    "            b = lda_mat.loc[v].to_numpy() if v in lda_mat.index else np.zeros(len(topic_cols))\n",
    "            num = np.dot(a,b)\n",
    "            den = np.linalg.norm(a)*np.linalg.norm(b)+1e-8\n",
    "            sim[i] = num/den\n",
    "        self._flush('lda_topics_similarity', sim)\n",
    "\n",
    "\n",
    "    # def compute_author_graph_heuristics(self):\n",
    "    #     pass\n",
    "    def compute_author_graph_heuristics(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute author-graph heuristics in batches:\n",
    "          - average common neighbors count per author-pair\n",
    "          - average Adamic–Adar per author-pair\n",
    "          - average Resource Allocation per author-pair\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Author-graph heuristics: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare ID arrays and result buffers\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        cn_arr = np.zeros(n, dtype=float)\n",
    "        aa_arr = np.zeros(n, dtype=float)\n",
    "        ra_arr = np.zeros(n, dtype=float)\n",
    "\n",
    "        # Local references for speed\n",
    "        G_auth = self.G_auth\n",
    "        deg_auth = dict(G_auth.degree())\n",
    "\n",
    "        # Process in batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_batch = u[start:end]\n",
    "            v_batch = v[start:end]\n",
    "\n",
    "            for i, (uid, vid) in enumerate(zip(u_batch, v_batch)):\n",
    "                # Get author lists for each paper\n",
    "                Au = self.paper_to_authors.get(uid, [])\n",
    "                Av = self.paper_to_authors.get(vid, [])\n",
    "                cn_vals, aa_vals, ra_vals = [], [], []\n",
    "\n",
    "                # Compute per-author-pair metrics\n",
    "                for a, b in product(Au, Av):\n",
    "                    if G_auth.has_node(a) and G_auth.has_node(b):\n",
    "                        common = list(nx.common_neighbors(G_auth, a, b))\n",
    "                        if common:\n",
    "                            cn_vals.append(len(common))\n",
    "                            aa_vals.append(sum(1.0 / np.log(1 + deg_auth[z]) for z in common))\n",
    "                            ra_vals.append(sum(1.0 / deg_auth[z] for z in common))\n",
    "\n",
    "                # Aggregate (mean) or default to 0\n",
    "                idx = start + i\n",
    "                if cn_vals:\n",
    "                    cn_arr[idx] = np.mean(cn_vals)\n",
    "                    aa_arr[idx] = np.mean(aa_vals)\n",
    "                    ra_arr[idx] = np.mean(ra_vals)\n",
    "\n",
    "        # Flush to disk\n",
    "        # Stack all three author-graph features into one array (n_pairs × 3)\n",
    "        all_feats = np.column_stack([\n",
    "            cn_arr,\n",
    "            aa_arr,\n",
    "            ra_arr\n",
    "        ])\n",
    "        # Flush as a single file\n",
    "        self._flush('author_graph_heuristics', all_feats)\n",
    "        print(f\"INFO: Author-graph heuristics done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "    \n",
    "    # def compute_embedding_features(self):\n",
    "    #     pass\n",
    "    def compute_embedding_features(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute embedding-based scalars in batches:\n",
    "          - citation_node2vec_cosine, _dot, _l2\n",
    "          - citation_walklets_cosine, _dot, _l2\n",
    "          - author_node2vec_cosine, _dot, _l2\n",
    "          - author_walklets_cosine, _dot, _l2\n",
    "        Flushes all 12 arrays in a single .npz.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Embedding feats: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare id arrays\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # Allocate buffers\n",
    "        cnv_c_cos = np.zeros(n, dtype=float)\n",
    "        cnv_c_dot = np.zeros(n, dtype=float)\n",
    "        cnv_c_l2  = np.zeros(n, dtype=float)\n",
    "        cnv_w_cos = np.zeros(n, dtype=float)\n",
    "        cnv_w_dot = np.zeros(n, dtype=float)\n",
    "        cnv_w_l2  = np.zeros(n, dtype=float)\n",
    "        # Precompute author-level mean embeddings\n",
    "        # assume self.author_node2vec & self.author_walklets exist\n",
    "        # and self.paper_to_authors maps pid→list of a_ids\n",
    "        def mean_emb(pid, emb_dict, dim):\n",
    "            vs = [emb_dict[a] for a in self.paper_to_authors.get(pid, []) if a in emb_dict]\n",
    "            return np.mean(vs, axis=0) if vs else np.zeros(dim, dtype=float)\n",
    "        # determine dims\n",
    "        d_cn = next(iter(self.citation_node2vec.values())).shape[0]\n",
    "        d_aw = next(iter(self.citation_walklets.values())).shape[0]\n",
    "        d_an = next(iter(self.author_node2vec.values())).shape[0]\n",
    "        d_awl= next(iter(self.author_walklets.values())).shape[0]\n",
    "        # process in batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_b, v_b = u[start:end], v[start:end]\n",
    "            # citation-node2vec\n",
    "            Cv = np.vstack([ self.citation_node2vec.get(pid, np.zeros(d_cn)) for pid in u_b ])\n",
    "            Vv = np.vstack([ self.citation_node2vec.get(pid, np.zeros(d_cn)) for pid in v_b ])\n",
    "            dot = np.einsum('ij,ij->i', Cv, Vv)\n",
    "            nu  = np.linalg.norm(Cv, axis=1); nv = np.linalg.norm(Vv, axis=1)\n",
    "            cnv_c_cos[start:end] = dot / (nu*nv + 1e-8)\n",
    "            cnv_c_dot[start:end] = dot\n",
    "            cnv_c_l2[start:end]  = np.linalg.norm(Cv - Vv, axis=1)\n",
    "            # citation-walklets\n",
    "            Cw = np.vstack([ self.citation_walklets.get(pid, np.zeros(d_aw)) for pid in u_b ])\n",
    "            Wv = np.vstack([ self.citation_walklets.get(pid, np.zeros(d_aw)) for pid in v_b ])\n",
    "            dot = np.einsum('ij,ij->i', Cw, Wv)\n",
    "            nu  = np.linalg.norm(Cw, axis=1); nv = np.linalg.norm(Wv, axis=1)\n",
    "            cnv_w_cos[start:end] = dot / (nu*nv + 1e-8)\n",
    "            cnv_w_dot[start:end] = dot\n",
    "            cnv_w_l2[start:end]  = np.linalg.norm(Cw - Wv, axis=1)\n",
    "            # author-node2vec\n",
    "            An = np.vstack([ mean_emb(pid, self.author_node2vec, d_an) for pid in u_b ])\n",
    "            Bn = np.vstack([ mean_emb(pid, self.author_node2vec, d_an) for pid in v_b ])\n",
    "            dot = np.einsum('ij,ij->i', An, Bn)\n",
    "            nu  = np.linalg.norm(An, axis=1); nv = np.linalg.norm(Bn, axis=1)\n",
    "            # reuse buffers names for author if desired, or separate\n",
    "            # here stacking all into one npz with clear keys below\n",
    "            # similarly for author-walklets\n",
    "            Aw = np.vstack([ mean_emb(pid, self.author_walklets, d_awl) for pid in u_b ])\n",
    "            Bw = np.vstack([ mean_emb(pid, self.author_walklets, d_awl) for pid in v_b ])\n",
    "            dot_aw = np.einsum('ij,ij->i', Aw, Bw)\n",
    "            nu_aw  = np.linalg.norm(Aw, axis=1); nv_aw = np.linalg.norm(Bw, axis=1)\n",
    "            # store author embeddings\n",
    "            if start == 0:\n",
    "                an_cos = np.zeros(n, dtype=float)\n",
    "                an_dot = np.zeros(n, dtype=float)\n",
    "                an_l2  = np.zeros(n, dtype=float)\n",
    "                aw_cos = np.zeros(n, dtype=float)\n",
    "                aw_dot = np.zeros(n, dtype=float)\n",
    "                aw_l2  = np.zeros(n, dtype=float)\n",
    "            an_cos[start:end] = dot / (nu*nv + 1e-8)\n",
    "            an_dot[start:end] = dot\n",
    "            an_l2[start:end]  = np.linalg.norm(An - Bn, axis=1)\n",
    "            aw_cos[start:end] = dot_aw / (nu_aw*nv_aw + 1e-8)\n",
    "            aw_dot[start:end] = dot_aw\n",
    "            aw_l2[start:end]  = np.linalg.norm(Aw - Bw, axis=1)\n",
    "\n",
    "        # stack and flush all 12 features\n",
    "        np.savez(\n",
    "            os.path.join(self.output_dir, 'embedding_features.npz'),\n",
    "            citation_node2vec_cosine       = cnv_c_cos,\n",
    "            citation_node2vec_dot          = cnv_c_dot,\n",
    "            citation_node2vec_l2           = cnv_c_l2,\n",
    "            citation_walklets_cosine       = cnv_w_cos,\n",
    "            citation_walklets_dot          = cnv_w_dot,\n",
    "            citation_walklets_l2           = cnv_w_l2,\n",
    "            author_node2vec_cosine         = an_cos,\n",
    "            author_node2vec_dot            = an_dot,\n",
    "            author_node2vec_l2             = an_l2,\n",
    "            author_walklets_cosine         = aw_cos,\n",
    "            author_walklets_dot            = aw_dot,\n",
    "            author_walklets_l2             = aw_l2\n",
    "        )\n",
    "        print(f\"INFO: Embedding feats done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    def compute_author_aggregate_stats(self):\n",
    "        \"\"\"\n",
    "        Compute author-level aggregate stats for each pair:\n",
    "          - citing_author_mean_pagerank, cited_author_mean_pagerank\n",
    "          - citing_author_max_pagerank,  cited_author_max_pagerank\n",
    "          - citing_author_mean_degree,   cited_author_mean_degree\n",
    "        Results saved together in 'author_aggregate_stats.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Author agg stats: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Compute pagerank and degree on co-author graph\n",
    "        auth_pr  = nx.pagerank(self.G_auth, weight='weight')\n",
    "        auth_deg = dict(self.G_auth.degree())\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        mean_pr_u = np.zeros(n, dtype=float)\n",
    "        mean_pr_v = np.zeros(n, dtype=float)\n",
    "        max_pr_u  = np.zeros(n, dtype=float)\n",
    "        max_pr_v  = np.zeros(n, dtype=float)\n",
    "        mean_deg_u = np.zeros(n, dtype=float)\n",
    "        mean_deg_v = np.zeros(n, dtype=float)\n",
    "\n",
    "        for idx, (uid, vid) in enumerate(zip(u, v)):\n",
    "            Au = self.paper_to_authors.get(uid, [])\n",
    "            Av = self.paper_to_authors.get(vid, [])\n",
    "            # pagerank stats\n",
    "            prs_u = [auth_pr.get(a, 0.0) for a in Au]\n",
    "            prs_v = [auth_pr.get(a, 0.0) for a in Av]\n",
    "            if prs_u:\n",
    "                mean_pr_u[idx] = sum(prs_u) / len(prs_u)\n",
    "                max_pr_u[idx]  = max(prs_u)\n",
    "            if prs_v:\n",
    "                mean_pr_v[idx] = sum(prs_v) / len(prs_v)\n",
    "                max_pr_v[idx]  = max(prs_v)\n",
    "            # degree stats\n",
    "            degs_u = [auth_deg.get(a, 0) for a in Au]\n",
    "            degs_v = [auth_deg.get(a, 0) for a in Av]\n",
    "            if degs_u:\n",
    "                mean_deg_u[idx] = sum(degs_u) / len(degs_u)\n",
    "            if degs_v:\n",
    "                mean_deg_v[idx] = sum(degs_v) / len(degs_v)\n",
    "\n",
    "        # Flush all six stats together\n",
    "        out_path = os.path.join(self.output_dir, 'author_aggregate_stats.npz')\n",
    "        np.savez(\n",
    "            out_path,\n",
    "            citing_author_mean_pagerank = mean_pr_u,\n",
    "            cited_author_mean_pagerank  = mean_pr_v,\n",
    "            citing_author_max_pagerank  = max_pr_u,\n",
    "            cited_author_max_pagerank   = max_pr_v,\n",
    "            citing_author_mean_degree   = mean_deg_u,\n",
    "            cited_author_mean_degree    = mean_deg_v\n",
    "        )\n",
    "        print(f\"INFO: Author agg stats done in {time.time() - t0:.1f}s, flushed to {out_path}\")\n",
    "\n",
    "\n",
    "     # def compute_coauthor_distance(self):\n",
    "    #     pass\n",
    "    def compute_coauthor_distance(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute co-author distance metrics in batches:\n",
    "          - coauth_min_dist: minimum shortest-path between any author of citing & cited\n",
    "          - coauth_mean_dist: average such shortest-path\n",
    "          - coauth_max_dist: maximum such shortest-path\n",
    "          - coauth_inv_min: 1 / (min_dist + 1)\n",
    "          - coauth_close_bin: binary flag if min_dist <= 2\n",
    "        Flushes all five arrays as a single .npz.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Co-author distance: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Prepare id arrays and result buffers\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        min_arr  = np.zeros(n, dtype=float)\n",
    "        mean_arr = np.zeros(n, dtype=float)\n",
    "        max_arr  = np.zeros(n, dtype=float)\n",
    "\n",
    "        # Maximum distance if no path exists\n",
    "        max_dist = self.G_auth.number_of_nodes()\n",
    "\n",
    "        # Process in batches\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            u_batch = u[start:end]\n",
    "            v_batch = v[start:end]\n",
    "\n",
    "            for i, (uid, vid) in enumerate(zip(u_batch, v_batch)):\n",
    "                Au = self.paper_to_authors.get(uid, [])\n",
    "                Av = self.paper_to_authors.get(vid, [])\n",
    "                dists = []\n",
    "                for a in Au:\n",
    "                    for b in Av:\n",
    "                        if self.G_auth.has_node(a) and self.G_auth.has_node(b):\n",
    "                            try:\n",
    "                                dists.append(nx.shortest_path_length(self.G_auth, a, b))\n",
    "                            except nx.NetworkXNoPath:\n",
    "                                dists.append(max_dist)\n",
    "                idx = start + i\n",
    "                if dists:\n",
    "                    min_arr[idx]  = min(dists)\n",
    "                    mean_arr[idx] = sum(dists) / len(dists)\n",
    "                    max_arr[idx]  = max(dists)\n",
    "                else:\n",
    "                    min_arr[idx] = mean_arr[idx] = max_arr[idx] = max_dist\n",
    "\n",
    "        # Derived metrics\n",
    "        inv_min  = 1.0 / (min_arr + 1.0)\n",
    "        close_bin = (min_arr <= 2).astype(int)\n",
    "\n",
    "        # Flush all features together\n",
    "        out_path = os.path.join(self.output_dir, 'coauthor_distance.npz')\n",
    "        np.savez(\n",
    "            out_path,\n",
    "            coauth_min_dist  = min_arr,\n",
    "            coauth_mean_dist = mean_arr,\n",
    "            coauth_max_dist  = max_arr,\n",
    "            coauth_inv_min   = inv_min,\n",
    "            coauth_close_bin = close_bin\n",
    "        )\n",
    "        print(f\"INFO: Co-author distance done in {time.time() - t0:.1f}s, flushed to {out_path}\")\n",
    "\n",
    "\n",
    "    def compute_node_level_metrics(self):\n",
    "        \"\"\"\n",
    "        Compute node‐level graph features for each (citing, cited) pair:\n",
    "          - citing_in_degree, citing_out_degree, citing_degree\n",
    "          - cited_in_degree,  cited_out_degree,  cited_degree\n",
    "          - citing_pagerank,   cited_pagerank\n",
    "          - citing_triangles,  cited_triangles\n",
    "          - citing_clustering, cited_clustering\n",
    "          - citing_core,       cited_core\n",
    "          - citing_onion,      cited_onion\n",
    "          - citing_eigen,      cited_eigen\n",
    "          - common_neighbors (undirected)\n",
    "        Flush all 21 arrays as one .npz.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Node‐level metrics: {n} pairs (~{n/self.its:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Build undirected version\n",
    "        und = self.G.to_undirected()\n",
    "\n",
    "        # Compute raw dicts\n",
    "        in_deg   = dict(self.G.in_degree())\n",
    "        out_deg  = dict(self.G.out_degree())\n",
    "        deg      = dict(self.G.degree())\n",
    "        pr       = nx.pagerank(self.G, weight=None)\n",
    "        tri      = nx.triangles(und)\n",
    "        clust    = nx.clustering(und, weight=None)\n",
    "        core     = nx.core_number(und)\n",
    "        onion    = nx.onion_layers(und)\n",
    "        eig      = nx.eigenvector_centrality(self.G, max_iter=500)\n",
    "\n",
    "        # Prepare index arrays\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # Map into arrays\n",
    "        ci_deg = np.array([in_deg.get(x,0) for x in u])\n",
    "        co_deg = np.array([out_deg.get(x,0) for x in u])\n",
    "        ct_deg = np.array([deg.get(x,0) for x in u])\n",
    "        di_deg = np.array([in_deg.get(x,0) for x in v])\n",
    "        do_deg = np.array([out_deg.get(x,0) for x in v])\n",
    "        dt_deg = np.array([deg.get(x,0) for x in v])\n",
    "\n",
    "        ci_pr = np.array([pr.get(x,0.0) for x in u])\n",
    "        co_pr = np.array([pr.get(x,0.0) for x in v])\n",
    "\n",
    "        ci_tri = np.array([tri.get(x,0) for x in u])\n",
    "        co_tri = np.array([tri.get(x,0) for x in v])\n",
    "\n",
    "        ci_cl = np.array([clust.get(x,0.0) for x in u])\n",
    "        co_cl = np.array([clust.get(x,0.0) for x in v])\n",
    "\n",
    "        ci_co = np.array([core.get(x,0) for x in u])\n",
    "        co_co = np.array([core.get(x,0) for x in v])\n",
    "\n",
    "        ci_on = np.array([onion.get(x,0) for x in u])\n",
    "        co_on = np.array([onion.get(x,0) for x in v])\n",
    "\n",
    "        ci_ei = np.array([eig.get(x,0.0) for x in u])\n",
    "        co_ei = np.array([eig.get(x,0.0) for x in v])\n",
    "\n",
    "        # Common neighbors via adjacency-squared\n",
    "        nodes = list(und.nodes())\n",
    "        idx_map = {node:i for i,node in enumerate(nodes)}\n",
    "        A = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A)\n",
    "        ui = [idx_map[x] for x in u]\n",
    "        vi = [idx_map[x] for x in v]\n",
    "        cn = np.array(A2[ui, vi]).ravel()\n",
    "\n",
    "        # Stack and flush\n",
    "        np.savez(\n",
    "            os.path.join(self.output_dir, 'node_level_metrics.npz'),\n",
    "            citing_in_degree       = ci_deg,\n",
    "            citing_out_degree      = co_deg,\n",
    "            citing_degree          = ct_deg,\n",
    "            cited_in_degree        = di_deg,\n",
    "            cited_out_degree       = do_deg,\n",
    "            cited_degree           = dt_deg,\n",
    "            citing_pagerank        = ci_pr,\n",
    "            cited_pagerank         = co_pr,\n",
    "            citing_triangles       = ci_tri,\n",
    "            cited_triangles        = co_tri,\n",
    "            citing_clustering      = ci_cl,\n",
    "            cited_clustering       = co_cl,\n",
    "            citing_core_number     = ci_co,\n",
    "            cited_core_number      = co_co,\n",
    "            citing_onion_number    = ci_on,\n",
    "            cited_onion_number     = co_on,\n",
    "            citing_eigenvector     = ci_ei,\n",
    "            cited_eigenvector      = co_ei,\n",
    "            common_neighbors       = cn\n",
    "        )\n",
    "        print(f\"INFO: Node‐level done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "\n",
    "\n",
    "    def compute_pair_heuristics(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute pair‐level heuristics on the citation graph:\n",
    "          - citation_jaccard, salton, hub_depressed, adamic_adar\n",
    "          - preferential_attachment, resource_allocation\n",
    "          - directed_shortest_path (–1 if none)\n",
    "        Flush all 7 arrays in 'pair_heuristics.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Pair heuristics: {n} pairs (~{n/self.its:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        und = self.G.to_undirected()\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # Precompute deg and common_neighbors via A2\n",
    "        deg_dict = dict(self.G.degree())\n",
    "        ui = u; vi = v\n",
    "        nodes = list(und.nodes())\n",
    "        idx_map = {node:i for i,node in enumerate(nodes)}\n",
    "        A = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A)\n",
    "        cn = np.array([A2[idx_map[x], idx_map[y]] for x,y in zip(u,v)])\n",
    "\n",
    "        # Buffers\n",
    "        jacc = cn / (np.array([deg_dict.get(x,0) + deg_dict.get(y,0) - c for x,y,c in zip(u,v,cn)]) + 1e-8)\n",
    "        sal   = cn / np.sqrt(np.array([deg_dict.get(x,0)*deg_dict.get(y,0) for x,y in zip(u,v)]) + 1e-8)\n",
    "        hub   = cn / (np.maximum([deg_dict.get(x,0) for x in u],[deg_dict.get(y,0) for y in v]) + 1e-8)\n",
    "        # Adamic–Adar & RA\n",
    "        aa = np.zeros(n, dtype=float)\n",
    "        ra = np.zeros(n, dtype=float)\n",
    "        for i,(x,y) in enumerate(zip(u,v)):\n",
    "            aa[i] = sum(1.0/np.log(1+und.degree(z)) for z in nx.common_neighbors(und, x, y))\n",
    "            ra[i] = sum(1.0/und.degree(z)        for z in nx.common_neighbors(und, x, y))\n",
    "        # Preferential attachment\n",
    "        pa = np.array([self.G.out_degree(x)*self.G.in_degree(y) for x,y in zip(u,v)])\n",
    "        # Directed shortest paths (batch)\n",
    "        dsp = np.full(n, -1, dtype=int)\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start+batch_size, n)\n",
    "            for i,(x,y) in enumerate(zip(u[start:end],v[start:end])):\n",
    "                try:\n",
    "                    dsp[start+i] = nx.shortest_path_length(self.G, x, y)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    dsp[start+i] = -1\n",
    "\n",
    "        # Flush\n",
    "        np.savez(\n",
    "            os.path.join(self.output_dir,'pair_heuristics.npz'),\n",
    "            citation_G_jaccard              = jacc,\n",
    "            citation_G_salton               = sal,\n",
    "            citation_G_hub_depressed        = hub,\n",
    "            citation_G_adamic_adar          = aa,\n",
    "            citation_G_resource_allocation  = ra,\n",
    "            citation_G_preferential_attachment = pa,\n",
    "            citation_G_sp_directed          = dsp\n",
    "        )\n",
    "        print(f\"INFO: Pair heuristics done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    \n",
    "\n",
    "    def compute_path_based_scores(self, beta=0.005, epsilon=0.001):\n",
    "        \"\"\"\n",
    "        2. Path‐based Scores Beyond AA/RA:\n",
    "          • Katz_index ≈ β·A + β²·A2 + β³·A3\n",
    "          • Local Path index = A2 + ε·A3\n",
    "        Flushes both arrays in 'path_based_scores.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Path-based scores: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # α) build undirected adjacency powers\n",
    "        und = self.G.to_undirected()\n",
    "        nodes = list(und.nodes())\n",
    "        idx = {node:i for i,node in enumerate(nodes)}\n",
    "        A  = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A)\n",
    "        A3 = A2.dot(A)\n",
    "\n",
    "        # β) buffers\n",
    "        katz = np.zeros(n, dtype=float)\n",
    "        lp   = np.zeros(n, dtype=float)\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # γ) extract per‐pair\n",
    "        for i,(x,y) in enumerate(zip(u,v)):\n",
    "            xi, yi = idx[x], idx[y]\n",
    "            katz[i] = beta * A[xi,yi]       \\\n",
    "                    + (beta**2) * A2[xi,yi] \\\n",
    "                    + (beta**3) * A3[xi,yi]\n",
    "            lp[i]   = A2[xi,yi] + epsilon * A3[xi,yi]\n",
    "\n",
    "        # δ) flush\n",
    "        out = os.path.join(self.output_dir, 'path_based_scores.npz')\n",
    "        np.savez(out, katz_index=katz, local_path=lp)\n",
    "        print(f\"INFO: Path‐based done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "\n",
    "    def compute_community_features(self):\n",
    "        \"\"\"\n",
    "        3. Community‐Based Features on citation graph:\n",
    "          • same_community: 1 if both in same Louvain community\n",
    "          • comm_size_ratio: |C_u| / |C_v|\n",
    "        Flushes both arrays in 'community_features.npz'.\n",
    "        \"\"\"\n",
    "        print(\"INFO: Detecting communities (greedy modularity)...\")\n",
    "        t0 = time.time()\n",
    "        und = self.G.to_undirected()\n",
    "        comms = nx.algorithms.community.greedy_modularity_communities(und)\n",
    "        comm_map = {node:cid for cid,comm in enumerate(comms) for node in comm}\n",
    "        sizes = {cid: len(comm) for cid,comm in enumerate(comms)}\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        n = len(u)\n",
    "        same  = np.zeros(n, dtype=int)\n",
    "        ratio = np.zeros(n, dtype=float)\n",
    "\n",
    "        for i,(x,y) in enumerate(zip(u,v)):\n",
    "            cx, cy = comm_map.get(x,-1), comm_map.get(y,-1)\n",
    "            same[i] = int(cx==cy and cx!=-1)\n",
    "            if cx in sizes and cy in sizes and sizes[cy]>0:\n",
    "                ratio[i] = sizes[cx] / sizes[cy]\n",
    "\n",
    "        out = os.path.join(self.output_dir, 'community_features.npz')\n",
    "        np.savez(out, same_community=same, comm_size_ratio=ratio)\n",
    "        print(f\"INFO: Community done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "\n",
    "    def compute_motif_counts(self):\n",
    "        \"\"\"\n",
    "        4. Higher‐Order Motif Counts:\n",
    "          • triangles_through_edge = A2[x,y]//2\n",
    "          • cycles4 ≈ number of paths length‐3 = A3[x,y]\n",
    "        Flushes both arrays in 'motif_counts.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Motif counts: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        und = self.G.to_undirected()\n",
    "        nodes = list(und.nodes()); idx = {node:i for i,node in enumerate(nodes)}\n",
    "        A  = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A); A3 = A2.dot(A)\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        tri  = np.zeros(n, dtype=int)\n",
    "        cyc4 = np.zeros(n, dtype=float)\n",
    "\n",
    "        for i,(x,y) in enumerate(zip(u,v)):\n",
    "            xi, yi = idx[x], idx[y]\n",
    "            tri[i]  = int(A2[xi,yi] // 2)\n",
    "            cyc4[i] = A3[xi,yi]\n",
    "\n",
    "        out = os.path.join(self.output_dir, 'motif_counts.npz')\n",
    "        np.savez(out, triangles=tri, cycles4=cyc4)\n",
    "        print(f\"INFO: Motifs done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "\n",
    "    def extract_keywords_from_abstracts(self, abstracts_path, top_k=10, max_features=5000):\n",
    "        \"\"\"\n",
    "        5a. Extract top-k keywords per paper from abstracts via TF–IDF.\n",
    "        Stores self.paper_keywords: {paper_id: [kw1,…,kw_topk]}.\n",
    "        \"\"\"\n",
    "        print(\"INFO: Extracting keywords via TF–IDF from abstracts...\")\n",
    "        t0 = time.time()\n",
    "        # load abstracts parquet with columns ['paper_id','abstract']\n",
    "        df_abs = pd.read_parquet(abstracts_path)\n",
    "        texts  = df_abs['abstract'].fillna(\"\").tolist()\n",
    "        pids   = df_abs['paper_id'].tolist()\n",
    "\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vec = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        X = vec.fit_transform(texts)  # shape = (n_papers, max_features)\n",
    "        features = vec.get_feature_names_out()\n",
    "\n",
    "        self.paper_keywords = {}\n",
    "        for i, pid in enumerate(pids):\n",
    "            row = X[i].tocoo()\n",
    "            if row.nnz:\n",
    "                top_idx = np.argsort(row.data)[-top_k:]\n",
    "                kws = [features[row.col[j]] for j in top_idx]\n",
    "            else:\n",
    "                kws = []\n",
    "            self.paper_keywords[pid] = kws\n",
    "\n",
    "        print(f\"INFO: Keywords extracted for {len(pids)} papers in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def compute_content_overlap(self):\n",
    "        \"\"\"\n",
    "        5b. Content Overlap Beyond Abstracts:\n",
    "          • title_jaccard: (if titles exist)\n",
    "          • keyword_jaccard: Jaccard of extracted keywords\n",
    "        Requires run of extract_keywords_from_abstracts(...) first.\n",
    "        Flushes 'content_overlap.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Content overlap: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # assume self.paper_keywords exists\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "        kw_j = np.zeros(n, dtype=float)\n",
    "\n",
    "        for i,(x,y) in enumerate(zip(u,v)):\n",
    "            Au = set(self.paper_keywords.get(x, []))\n",
    "            Av = set(self.paper_keywords.get(y, []))\n",
    "            inter = Au & Av\n",
    "            uni   = Au | Av\n",
    "            kw_j[i] = len(inter) / (len(uni) + 1e-8)\n",
    "\n",
    "        out = os.path.join(self.output_dir, 'content_overlap.npz')\n",
    "        np.savez(out, keyword_jaccard=kw_j)\n",
    "        print(f\"INFO: Content done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "    def compute_author_domain_similarity(self, batch_size=100000):\n",
    "        \"\"\"\n",
    "        Compute author-domain similarity features:\n",
    "          • max_citing_topic_author_dom_cosine\n",
    "          • mean_citing_topic_author_dom_cosine\n",
    "          • min_citing_topic_author_dom_cosine\n",
    "          • max_cited_topic_author_dom_cosine\n",
    "          • mean_cited_topic_author_dom_cosine\n",
    "          • min_cited_topic_author_dom_cosine\n",
    "        Flushes all 6 arrays in 'author_domain_similarity.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        est = n / self.its\n",
    "        print(f\"INFO: Author-domain sim: {n} pairs (~{est:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        # prepare default zero‐vector for missing topics\n",
    "        D_topic = next(iter(self.topic_dist_arr.values())).shape[0]\n",
    "        zero_vec = np.zeros(D_topic, dtype=float)\n",
    "\n",
    "        # result buffers\n",
    "        max_u = np.zeros(n, float)\n",
    "        mean_u = np.zeros(n, float)\n",
    "        min_u = np.zeros(n, float)\n",
    "        max_v = np.zeros(n, float)\n",
    "        mean_v = np.zeros(n, float)\n",
    "        min_v = np.zeros(n, float)\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # batch over pairs\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            for i, (uid, vid) in enumerate(zip(u[start:end], v[start:end])):\n",
    "                idx = start + i\n",
    "\n",
    "                # domain vector for cited (used by citing→authors)\n",
    "                dom_v = self.topic_dist_arr.get(vid, zero_vec)\n",
    "                sims_u = []\n",
    "                for a in self.paper_to_authors.get(uid, []):\n",
    "                    vec_a = self.auth_topic_dict.get(a)\n",
    "                    if vec_a is not None:\n",
    "                        sims_u.append(\n",
    "                            float(\n",
    "                                np.dot(vec_a, dom_v) /\n",
    "                                (np.linalg.norm(vec_a) * np.linalg.norm(dom_v) + 1e-8)\n",
    "                            )\n",
    "                        )\n",
    "                if sims_u:\n",
    "                    max_u[idx], mean_u[idx], min_u[idx] = max(sims_u), np.mean(sims_u), min(sims_u)\n",
    "\n",
    "                # domain vector for citing (used by cited→authors)\n",
    "                dom_u = self.topic_dist_arr.get(uid, zero_vec)\n",
    "                sims_v = []\n",
    "                for a in self.paper_to_authors.get(vid, []):\n",
    "                    vec_a = self.auth_topic_dict.get(a)\n",
    "                    if vec_a is not None:\n",
    "                        sims_v.append(\n",
    "                            float(\n",
    "                                np.dot(vec_a, dom_u) /\n",
    "                                (np.linalg.norm(vec_a) * np.linalg.norm(dom_u) + 1e-8)\n",
    "                            )\n",
    "                        )\n",
    "                if sims_v:\n",
    "                    max_v[idx], mean_v[idx], min_v[idx] = max(sims_v), np.mean(sims_v), min(sims_v)\n",
    "\n",
    "        # flush to disk\n",
    "        out = os.path.join(self.output_dir, 'author_domain_similarity.npz')\n",
    "        np.savez(\n",
    "            out,\n",
    "            max_citing_topic_author_dom_cosine   = max_u,\n",
    "            mean_citing_topic_author_dom_cosine  = mean_u,\n",
    "            min_citing_topic_author_dom_cosine   = min_u,\n",
    "            max_cited_topic_author_dom_cosine    = max_v,\n",
    "            mean_cited_topic_author_dom_cosine   = mean_v,\n",
    "            min_cited_topic_author_dom_cosine    = min_v\n",
    "        )\n",
    "        print(f\"INFO: Author-domain done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "        \n",
    "    def compute_paper_author_domain_similarity(self):\n",
    "        \"\"\"\n",
    "        Compute paper‐level author‐domain similarity:\n",
    "          • paper_total_auth_dom_cosine_citing_vs_cited\n",
    "          • paper_total_auth_dom_dot_citing_vs_cited\n",
    "          • paper_total_auth_dom_l2_citing_vs_cited\n",
    "        Uses self.paper_dom_dict.\n",
    "        Flushes 3 arrays in 'paper_author_domain.npz'.\n",
    "        \"\"\"\n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Paper–author‐domain sim: {n} pairs (~{n/self.its:.1f}s)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "\n",
    "        # build matrices\n",
    "        D = next(iter(self.paper_dom_dict.values())).shape[0]\n",
    "        U = np.vstack([ self.paper_dom_dict.get(pid, np.zeros(D)) for pid in u ])\n",
    "        V = np.vstack([ self.paper_dom_dict.get(pid, np.zeros(D)) for pid in v ])\n",
    "\n",
    "        dot = np.einsum('ij,ij->i', U, V)\n",
    "        nu  = np.linalg.norm(U, axis=1)\n",
    "        nv  = np.linalg.norm(V, axis=1)\n",
    "        cos = dot / (nu * nv + 1e-8)\n",
    "        l2  = np.linalg.norm(U - V, axis=1)\n",
    "\n",
    "        out = os.path.join(self.output_dir, 'paper_author_domain.npz')\n",
    "        np.savez(\n",
    "            out,\n",
    "            paper_total_auth_dom_cosine_citing_vs_cited = cos,\n",
    "            paper_total_auth_dom_dot_citing_vs_cited    = dot,\n",
    "            paper_total_auth_dom_l2_citing_vs_cited     = l2\n",
    "        )\n",
    "        print(f\"INFO: Paper–author‐domain done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "        \n",
    "\n",
    "    def compute_abstract_author_svd_similarity(self):\n",
    "        n = len(self.df_pairs)\n",
    "        dot_abs = np.zeros(n); cos_abs = np.zeros(n)\n",
    "        dot_auth= np.zeros(n); cos_auth= np.zeros(n)\n",
    "    \n",
    "        u = self.df_pairs.citing.to_numpy()\n",
    "        v = self.df_pairs.cited.to_numpy()\n",
    "    \n",
    "        for i,(uid,vid) in enumerate(zip(u,v)):\n",
    "            idx_u = self.pid_to_idx.get(uid, -1)\n",
    "            idx_v = self.pid_to_idx.get(vid, -1)\n",
    "            eu = self.emb_abs[idx_u] if idx_u>=0 else np.zeros(self.emb_abs.shape[1])\n",
    "            ev = self.emb_abs[idx_v] if idx_v>=0 else np.zeros(self.emb_abs.shape[1])\n",
    "            da = eu.dot(ev)\n",
    "            na = np.linalg.norm(eu); nb = np.linalg.norm(ev)\n",
    "            dot_abs[i] = da\n",
    "            cos_abs[i] = da/(na*nb+1e-8)\n",
    "    \n",
    "            # αν έχεις emb_auth\n",
    "            au = self.emb_auth.get(uid, np.zeros_like(eu))\n",
    "            av = self.emb_auth.get(vid, np.zeros_like(eu))\n",
    "            du = au.dot(av)\n",
    "            nu = np.linalg.norm(au); nv = np.linalg.norm(av)\n",
    "            dot_auth[i] = du\n",
    "            cos_auth[i] = du/(nu*nv+1e-8)\n",
    "    \n",
    "        out = os.path.join(self.output_dir,'svd_text_author.npz')\n",
    "        np.savez(out,\n",
    "                 cosine_abs_svd_koz  = cos_abs,\n",
    "                 cosine_auth_svd_koz = cos_auth)\n",
    "        print(f\"INFO: SVD text/author done.\")\n",
    "        \n",
    "    \n",
    "    def compute_rooted_pagerank(self, alpha=0.85, max_iter=100, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Compute Personalized (Rooted) PageRank score from each citing u to cited v,\n",
    "        grouping by u to avoid OOM and redundant computation:\n",
    "          • For each unique u, run PageRank on self.G with teleport vector focused on u\n",
    "          • Record the PageRank score at v for every (u,v) pair\n",
    "        Flushes array 'rooted_pagerank_score.npy'.\n",
    "        \"\"\"\n",
    "        n   = len(self.df_pairs)\n",
    "        # Estimate time assuming each PR is ~10× heavier than a single it/sec\n",
    "        est = n / (self.its / 10)\n",
    "        print(f\"INFO: Rooted PageRank: {n} pairs est ~{est:.1f}s\")\n",
    "        t0  = time.time()\n",
    "\n",
    "        # Prepare output buffer\n",
    "        scores = np.zeros(n, dtype=float)\n",
    "\n",
    "        # Group indices by citing‐paper u\n",
    "        from collections import defaultdict\n",
    "        idxs_by_u = defaultdict(list)\n",
    "        for idx, (u, v) in enumerate(zip(self.df_pairs.citing, self.df_pairs.cited)):\n",
    "            idxs_by_u[u].append((idx, v))\n",
    "\n",
    "        # Compute PageRank once per unique u, assign and free memory immediately\n",
    "        for u, lst in idxs_by_u.items():\n",
    "            # Build personalization vector: teleport only to u\n",
    "            pers = {node: 0.0 for node in self.G.nodes()}\n",
    "            pers[u] = 1.0\n",
    "            pr_u = nx.pagerank(\n",
    "                G=self.G,\n",
    "                alpha=alpha,\n",
    "                personalization=pers,\n",
    "                max_iter=max_iter,\n",
    "                tol=tol,\n",
    "                weight=None\n",
    "            )\n",
    "            # Assign scores for all (idx, v) belonging to this u\n",
    "            for idx, v in lst:\n",
    "                scores[idx] = pr_u.get(v, 0.0)\n",
    "            # Free the PageRank vector before next u\n",
    "            del pr_u\n",
    "\n",
    "        # Flush to disk\n",
    "        out = os.path.join(self.output_dir, 'rooted_pagerank_score.npy')\n",
    "        np.save(out, scores)\n",
    "        print(f\"INFO: Rooted PageRank done in {time.time()-t0:.1f}s, flushed to {out}\")\n",
    "\n",
    "    def compute_motif_and_path_features(self, beta=0.005, epsilon=0.001, batch_size=100_000):\n",
    "        \"\"\"\n",
    "        Memory‐safe, chunked computation of:\n",
    "          • katz_index            = β²·A² + β³·A³\n",
    "          • local_path            = A² + ε·A³\n",
    "          • triangles_through_edge= A²[u,v] // 2\n",
    "          • cycles4               = A³[u,v]\n",
    "          • citation_G_sp_directed= 2 if A²>0, 3 if A³>0, else -1\n",
    "    \n",
    "        Uses np.memmap to avoid holding all features in RAM,\n",
    "        and processes the pair list in chunks.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import time\n",
    "        import numpy as np\n",
    "        from numpy.lib.format import open_memmap\n",
    "    \n",
    "        n = len(self.df_pairs)\n",
    "        print(f\"INFO: Motif+Path feats (chunked): {n} pairs\")\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # — Build undirected adjacency and its powers once —\n",
    "        und = self.G.to_undirected()\n",
    "        nodes = list(und.nodes())\n",
    "        idx_map = {node: i for i, node in enumerate(nodes)}\n",
    "        A  = nx.to_scipy_sparse_matrix(und, nodes, format='csr')\n",
    "        A2 = A.dot(A)\n",
    "        A3 = A2.dot(A)\n",
    "    \n",
    "        # — Prepare on‐disk arrays via memmap —\n",
    "        out_dir = self.output_dir\n",
    "        katz_mmap = open_memmap(os.path.join(out_dir, 'katz_index_h2.npy'),\n",
    "                            mode='w+', dtype='float32', shape=(n,))\n",
    "        lp_mmap   = open_memmap(os.path.join(out_dir, 'local_path_h2.npy'),\n",
    "                                mode='w+', dtype='float32', shape=(n,))\n",
    "        tri_mmap  = open_memmap(os.path.join(out_dir, 'triangles_through_edge_h2.npy'),\n",
    "                                mode='w+', dtype='int32',   shape=(n,))\n",
    "        cyc4_mmap = open_memmap(os.path.join(out_dir, 'cycles4_h2.npy'),\n",
    "                                mode='w+', dtype='int32',   shape=(n,))\n",
    "        spd_mmap  = open_memmap(os.path.join(out_dir, 'citation_G_sp_directed_h2.npy'),\n",
    "                                mode='w+', dtype='int8',    shape=(n,))\n",
    "      \n",
    "        \n",
    "    \n",
    "        # — Load u/v arrays once —\n",
    "        u_arr = self.df_pairs['citing'].to_numpy()\n",
    "        v_arr = self.df_pairs['cited'].to_numpy()\n",
    "    \n",
    "        # — Process in chunks —\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(n, start + batch_size)\n",
    "            for i in range(start, end):\n",
    "                u, v = u_arr[i], v_arr[i]\n",
    "                ui, vi = idx_map[u], idx_map[v]\n",
    "                a2 = A2[ui, vi]\n",
    "                a3 = A3[ui, vi]\n",
    "    \n",
    "                katz_mmap[i] = (beta**2) * a2 + (beta**3) * a3\n",
    "                lp_mmap[i]   = a2 + epsilon * a3\n",
    "                tri_mmap[i]  = int(a2 // 2)\n",
    "                cyc4_mmap[i] = int(a3)\n",
    "                if a2 > 0:\n",
    "                    spd_mmap[i] = 2\n",
    "                elif a3 > 0:\n",
    "                    spd_mmap[i] = 3\n",
    "                # else leaves -1\n",
    "    \n",
    "            print(f\"  • Processed rows {start}–{end} in {time.time()-t0:.1f}s\")\n",
    "    \n",
    "        # — flush & cleanup memmaps —\n",
    "        del katz_mmap, lp_mmap, tri_mmap, cyc4_mmap, spd_mmap\n",
    "\n",
    "        print(f\"INFO: Motif+Path done in {time.time()-t0:.1f}s \")\n",
    "\n",
    "     ############################################################## EMBEDDINGS FEATURES #########################################################\n",
    "    def compute_specter_hadamard(self):\n",
    "        \"\"\"\n",
    "        Για κάθε (citing, cited) ζεύγος στο train split υπολογίζει\n",
    "        το element-wise Hadamard product των Specter embeddings\n",
    "        και το αποθηκεύει σε NumPy array shape = (n_train_pairs, D).\n",
    "        Επιστρέφει το array.\n",
    "        \"\"\"\n",
    "        # 1) Φόρτωση Specter embeddings\n",
    "        embeds = np.load(self.abstracts_emb_path)  # shape = [n_papers, D]\n",
    "        D = embeds.shape[1]\n",
    "\n",
    "        # 2) Επιλογή μόνο των train ζευγών\n",
    "        train_df = self.df_pairs\n",
    "        u_ids = train_df['citing'].astype(int).to_numpy()\n",
    "        v_ids = train_df['cited'].astype(int).to_numpy()\n",
    "        n = len(train_df)\n",
    "\n",
    "        # 3) Πρίζουμε πίνακα για τα Hadamard features\n",
    "        hadamard = np.zeros((n, D), dtype=float)\n",
    "\n",
    "        # 4) Υπολογισμός element-wise product για κάθε ζεύγος\n",
    "        for i, (u, v) in enumerate(zip(u_ids, v_ids)):\n",
    "            hadamard[i, :] = embeds[u] * embeds[v]\n",
    "\n",
    "        # 5) Αποθήκευση σε .npy\n",
    "        out_path = os.path.join(self.output_dir, 'specter_hadamard.npy')\n",
    "        np.save(out_path, hadamard)\n",
    "        print(f\"INFO: Saved Specter Hadamard features to {out_path} (shape={hadamard.shape})\")\n",
    "\n",
    "        return hadamard\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8a60046-5e57-4f82-b082-97811cb9fcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Loading test data…\n",
      "INFO: Loaded 106692 test pairs in 0.0s\n",
      "INFO: Loaded authors mapping (138499 papers)\n",
      "INFO: Loaded abstracts_embeds (138499, 300)\n",
      "INFO: Loaded pid_to_idx (138499 entries)\n",
      "INFO: Loaded Specter embeddings array (138499, 768)\n",
      "INFO: Loaded SciBERT embeddings (138499)\n",
      "INFO: Loaded LDA topics (131250) in 1.6s total\n",
      "INFO: Loaded graph embeddings: citation_node2vec=138499, citation_walklets=138499, author_node2vec=136863, author_walklets=136863\n",
      "INFO: BERTopic dicts ready (topics=131250, authors=133493, papers_dom=138192)\n",
      "INFO: Loaded SVD embeddings (abstracts=138499, authors=138499)\n",
      "INFO: Built citation graph (nodes=99760, edges=0)\n",
      "INFO: Built co-author graph (nodes=136863, edges=520007)\n",
      "✅ Data loaded, graphs ready.\n",
      "INFO: Saved Specter Hadamard features to D:/NLP/Features_XL/test\\specter_hadamard.npy (shape=(106692, 300))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.09443619e-01, -2.59840318e-02,  6.74534152e-02, ...,\n",
       "         3.23523901e-04, -3.45791189e-04, -1.65615114e-03],\n",
       "       [ 0.00000000e+00, -0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -0.00000000e+00,  0.00000000e+00, -0.00000000e+00],\n",
       "       [ 1.43329907e-01,  3.82459367e-03, -1.04114404e-02, ...,\n",
       "         1.74879250e-04, -1.84282744e-03, -1.03489366e-03],\n",
       "       ...,\n",
       "       [ 9.19556235e-02,  5.87895661e-03, -1.69018526e-02, ...,\n",
       "         6.64736521e-04,  9.31989034e-04,  1.07722241e-03],\n",
       "       [ 6.10241669e-02,  7.37106453e-03,  8.96277853e-03, ...,\n",
       "        -8.63924838e-04, -8.96194906e-05, -1.55157385e-06],\n",
       "       [ 8.38653397e-02, -1.58332135e-02,  6.22138566e-03, ...,\n",
       "         1.17844388e-03, -1.80144037e-04, -1.15641253e-04]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Δημιουργία extractor για test\n",
    "fe = TestFeatureExtractor(base_path=\"D:/NLP/tfidf_xgboost\",\n",
    "                          output_dir=\"D:/NLP/Features_XL/test\",\n",
    "                          chunked=True,\n",
    "                          its=200000)\n",
    "\n",
    "# 2) Φόρτωμα των test pairs\n",
    "fe.load_data(test_pairs_path=r\"C:\\Users\\mysmu\\Desktop\\Natural Language Processing\\nlp-cse-uoi-2025\\data_new\\test.txt\")\n",
    "\n",
    "# 3) Κλήση όλων των compute_... μεθόδων\n",
    "\n",
    "#fe.compute_tfidf_similarity()\n",
    "#fe.compute_specter_similarity()\n",
    "\n",
    "# fe.compute_co_citation_bibliographic()\n",
    "# fe.compute_author_overlap_jaccard()\n",
    "# fe.compute_pair_heuristics()\n",
    "# fe.compute_lda_topics_features()\n",
    "\n",
    "#fe.compute_community_features()       # same_community, comm_size_ratio\n",
    "#fe.compute_motif_counts()\n",
    "#fe.extract_keywords_from_abstracts(\"D:/NLP/tfidf_xgboost/cleaned_abstracts.parquet\", top_k=10)\n",
    "#fe.compute_content_overlap()\n",
    "\n",
    "#fe.compute_node_level_metrics()       # citing_degree + raw common_neighbors\n",
    "\n",
    "#fe.compute_motif_and_path_features()\n",
    "#fe.compute_abstract_author_svd_similarity()\n",
    "\n",
    "# fe.compute_scibert_similarity()\n",
    "# fe.compute_bertopic_features()\n",
    "# fe.compute_paper_author_domain_similarity()\n",
    "# fe.compute_author_domain_similarity()\n",
    "# fe.compute_author_aggregate_stats()\n",
    "# fe.compute_coauthor_distance()\n",
    "# fe.compute_author_graph_heuristics()\n",
    "\n",
    "#fe.compute_embedding_features()\n",
    "# fe.compute_rooted_pagerank()\n",
    "\n",
    " ############################################################## EMBEDDINGS FEATURES #########################################################\n",
    "fe.compute_specter_hadamard()\n",
    "\n",
    "# Τώρα μέσα στον φάκελο output_dir θα έχεις όλα τα .npy/.npz με τα test-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b77e9f-b148-419f-8d3a-960c4ab33e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
